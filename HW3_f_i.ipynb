{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW3\n",
    "f\n",
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE : An assumption\n",
    "In multiclass classification, we obtain a matrix of Beta's\n",
    "Thus we get a matrix of overall 'parameters' despite our original vector of predictors as with each predictor, with each class a parameter is associated\n",
    "\n",
    "THUS,\n",
    "We won't attempt to select features in this case for each split. If parameter of a feature corresponding to one class is zero, the parameter corresponding to other class may not be. As such we could write a code to check if a feature has parameter zero in every class, if it does, then obviously it is to be dropped, however this does complify the problem\n",
    "\n",
    "\n",
    "Thus L1-penalization will internally perform 'parameter-selection'\n",
    "We could perform, based on the obtained parameters, 'feature elimination' and 'non-zero feature identification', however, this, here won't be performed\n",
    "\n",
    "Thus we'll work with our base error_test_estimate vector and select the best l using the vector index where the error gets minimized. Then for that l we'll obtain the test set and obtain the actual test error, only and only for that l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DHRUV\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\DHRUV\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\DHRUV\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\DHRUV\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\DHRUV\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\DHRUV\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\DHRUV\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\DHRUV\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\DHRUV\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\DHRUV\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test error estimates : \n",
      "    Estimated Test Error\n",
      "1               15.9341\n",
      "2               13.0769\n",
      "3               11.5385\n",
      "4               24.7253\n",
      "5               17.4725\n",
      "6                21.978\n",
      "7                22.967\n",
      "8               24.6154\n",
      "9               24.5055\n",
      "10              30.3297\n",
      "11              23.0769\n",
      "12              23.4066\n",
      "13              30.5495\n",
      "14              27.8022\n",
      "15              27.4725\n",
      "16              23.2967\n",
      "17               31.978\n",
      "18               29.011\n",
      "19              27.4725\n",
      "20               26.044\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('bending1\\\\training\\\\dataset3.csv')\n",
    "df2 = pd.read_csv('bending1\\\\training\\\\dataset4.csv')\n",
    "df3 = pd.read_csv('bending1\\\\training\\\\dataset5.csv')\n",
    "df4 = pd.read_csv('bending1\\\\training\\\\dataset6.csv')\n",
    "df5 = pd.read_csv('bending1\\\\training\\\\dataset7.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df6 = pd.read_csv('bending2\\\\training\\\\dataset3.csv')\n",
    "df7 = pd.read_csv('bending2\\\\training\\\\dataset4.csv')\n",
    "df8 = pd.read_csv('bending2\\\\training\\\\dataset5.csv')\n",
    "df9 = pd.read_csv('bending2\\\\training\\\\dataset6.csv')\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "df10 = pd.read_csv('cycling\\\\training\\\\dataset4.csv')\n",
    "df11 = pd.read_csv('cycling\\\\training\\\\dataset5.csv')\n",
    "df12 = pd.read_csv('cycling\\\\training\\\\dataset6.csv')\n",
    "df13 = pd.read_csv('cycling\\\\training\\\\dataset7.csv')\n",
    "df14 = pd.read_csv('cycling\\\\training\\\\dataset8.csv')\n",
    "df15 = pd.read_csv('cycling\\\\training\\\\dataset9.csv')\n",
    "df16 = pd.read_csv('cycling\\\\training\\\\dataset10.csv')\n",
    "df17 = pd.read_csv('cycling\\\\training\\\\dataset11.csv')\n",
    "df18 = pd.read_csv('cycling\\\\training\\\\dataset12.csv')\n",
    "df19 = pd.read_csv('cycling\\\\training\\\\dataset13.csv')\n",
    "df20 = pd.read_csv('cycling\\\\training\\\\dataset14.csv')\n",
    "df21 = pd.read_csv('cycling\\\\training\\\\dataset15.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df22 = pd.read_csv('lying\\\\training\\\\dataset4.csv')\n",
    "df23 = pd.read_csv('lying\\\\training\\\\dataset5.csv')\n",
    "df24 = pd.read_csv('lying\\\\training\\\\dataset6.csv')\n",
    "df25 = pd.read_csv('lying\\\\training\\\\dataset7.csv')\n",
    "df26 = pd.read_csv('lying\\\\training\\\\dataset8.csv')\n",
    "df27 = pd.read_csv('lying\\\\training\\\\dataset9.csv')\n",
    "df28 = pd.read_csv('lying\\\\training\\\\dataset10.csv')\n",
    "df29 = pd.read_csv('lying\\\\training\\\\dataset11.csv')\n",
    "df30 = pd.read_csv('lying\\\\training\\\\dataset12.csv')\n",
    "df31 = pd.read_csv('lying\\\\training\\\\dataset13.csv')\n",
    "df32 = pd.read_csv('lying\\\\training\\\\dataset14.csv')\n",
    "df33 = pd.read_csv('lying\\\\training\\\\dataset15.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "df34 = pd.read_csv('sitting\\\\training\\\\dataset4.csv')\n",
    "df35 = pd.read_csv('sitting\\\\training\\\\dataset5.csv')\n",
    "df36 = pd.read_csv('sitting\\\\training\\\\dataset6.csv')\n",
    "df37 = pd.read_csv('sitting\\\\training\\\\dataset7.csv')\n",
    "df38 = pd.read_csv('sitting\\\\training\\\\dataset8.csv')\n",
    "df39 = pd.read_csv('sitting\\\\training\\\\dataset9.csv')\n",
    "df40 = pd.read_csv('sitting\\\\training\\\\dataset10.csv')\n",
    "df41 = pd.read_csv('sitting\\\\training\\\\dataset11.csv')\n",
    "df42 = pd.read_csv('sitting\\\\training\\\\dataset12.csv')\n",
    "df43 = pd.read_csv('sitting\\\\training\\\\dataset13.csv')\n",
    "df44 = pd.read_csv('sitting\\\\training\\\\dataset14.csv')\n",
    "df45 = pd.read_csv('sitting\\\\training\\\\dataset15.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df46 = pd.read_csv('standing\\\\training\\\\dataset4.csv')\n",
    "df47 = pd.read_csv('standing\\\\training\\\\dataset5.csv')\n",
    "df48 = pd.read_csv('standing\\\\training\\\\dataset6.csv')\n",
    "df49 = pd.read_csv('standing\\\\training\\\\dataset7.csv')\n",
    "df50 = pd.read_csv('standing\\\\training\\\\dataset8.csv')\n",
    "df51 = pd.read_csv('standing\\\\training\\\\dataset9.csv')\n",
    "df52 = pd.read_csv('standing\\\\training\\\\dataset10.csv')\n",
    "df53 = pd.read_csv('standing\\\\training\\\\dataset11.csv')\n",
    "df54 = pd.read_csv('standing\\\\training\\\\dataset12.csv')\n",
    "df55 = pd.read_csv('standing\\\\training\\\\dataset13.csv')\n",
    "df56 = pd.read_csv('standing\\\\training\\\\dataset14.csv')\n",
    "df57 = pd.read_csv('standing\\\\training\\\\dataset15.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df58 = pd.read_csv('walking\\\\training\\\\dataset4.csv')\n",
    "df59 = pd.read_csv('walking\\\\training\\\\dataset5.csv')\n",
    "df60 = pd.read_csv('walking\\\\training\\\\dataset6.csv')\n",
    "df61 = pd.read_csv('walking\\\\training\\\\dataset7.csv')\n",
    "df62 = pd.read_csv('walking\\\\training\\\\dataset8.csv')\n",
    "df63 = pd.read_csv('walking\\\\training\\\\dataset9.csv')\n",
    "df64 = pd.read_csv('walking\\\\training\\\\dataset10.csv')\n",
    "df65 = pd.read_csv('walking\\\\training\\\\dataset11.csv')\n",
    "df66 = pd.read_csv('walking\\\\training\\\\dataset12.csv')\n",
    "df67 = pd.read_csv('walking\\\\training\\\\dataset13.csv')\n",
    "df68 = pd.read_csv('walking\\\\training\\\\dataset14.csv')\n",
    "df69 = pd.read_csv('walking\\\\training\\\\dataset15.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df1 = df1.drop(df1.columns[0],axis=1)\n",
    "df2 = df2.drop(df2.columns[0],axis=1)\n",
    "df3 = df3.drop(df3.columns[0],axis=1)\n",
    "df4 = df4.drop(df4.columns[0],axis=1)\n",
    "df5 = df5.drop(df5.columns[0],axis=1)\n",
    "df6 = df6.drop(df6.columns[0],axis=1)\n",
    "df7 = df7.drop(df7.columns[0],axis=1)\n",
    "df8 = df8.drop(df8.columns[0],axis=1)\n",
    "df9 = df9.drop(df9.columns[0],axis=1)\n",
    "df10 = df10.drop(df10.columns[0],axis=1)\n",
    "df11 = df11.drop(df11.columns[0],axis=1)\n",
    "df12 = df12.drop(df12.columns[0],axis=1)\n",
    "df13 = df13.drop(df13.columns[0],axis=1)\n",
    "df14 = df14.drop(df14.columns[0],axis=1)\n",
    "df15 = df15.drop(df15.columns[0],axis=1)\n",
    "df16 = df16.drop(df16.columns[0],axis=1)\n",
    "df17 = df17.drop(df17.columns[0],axis=1)\n",
    "df18 = df18.drop(df18.columns[0],axis=1)\n",
    "df19 = df19.drop(df19.columns[0],axis=1)\n",
    "df20 = df20.drop(df20.columns[0],axis=1)\n",
    "df21 = df21.drop(df21.columns[0],axis=1)\n",
    "df22 = df22.drop(df22.columns[0],axis=1)\n",
    "df23 = df23.drop(df23.columns[0],axis=1)\n",
    "df24 = df24.drop(df24.columns[0],axis=1)\n",
    "df25 = df25.drop(df25.columns[0],axis=1)\n",
    "df26 = df26.drop(df26.columns[0],axis=1)\n",
    "df27 = df27.drop(df27.columns[0],axis=1)\n",
    "df28 = df28.drop(df28.columns[0],axis=1)\n",
    "df29 = df29.drop(df29.columns[0],axis=1)\n",
    "df30 = df30.drop(df30.columns[0],axis=1)\n",
    "df31 = df31.drop(df31.columns[0],axis=1)\n",
    "df32 = df32.drop(df32.columns[0],axis=1)\n",
    "df33 = df33.drop(df33.columns[0],axis=1)\n",
    "df34 = df34.drop(df34.columns[0],axis=1)\n",
    "df35 = df35.drop(df35.columns[0],axis=1)\n",
    "df36 = df36.drop(df36.columns[0],axis=1)\n",
    "df37 = df37.drop(df37.columns[0],axis=1)\n",
    "df38 = df38.drop(df38.columns[0],axis=1)\n",
    "df39 = df39.drop(df39.columns[0],axis=1)\n",
    "df40 = df40.drop(df40.columns[0],axis=1)\n",
    "df41 = df41.drop(df41.columns[0],axis=1)\n",
    "df42 = df42.drop(df42.columns[0],axis=1)\n",
    "df43 = df43.drop(df43.columns[0],axis=1)\n",
    "df44 = df44.drop(df44.columns[0],axis=1)\n",
    "df45 = df45.drop(df45.columns[0],axis=1)\n",
    "df46 = df46.drop(df46.columns[0],axis=1)\n",
    "df47 = df47.drop(df47.columns[0],axis=1)\n",
    "df48 = df48.drop(df48.columns[0],axis=1)\n",
    "df49 = df49.drop(df49.columns[0],axis=1)\n",
    "df50 = df50.drop(df50.columns[0],axis=1)\n",
    "df51 = df51.drop(df51.columns[0],axis=1)\n",
    "df52 = df52.drop(df52.columns[0],axis=1)\n",
    "df53 = df53.drop(df53.columns[0],axis=1)\n",
    "df54 = df54.drop(df54.columns[0],axis=1)\n",
    "df55 = df55.drop(df55.columns[0],axis=1)\n",
    "df56 = df56.drop(df56.columns[0],axis=1)\n",
    "df57 = df57.drop(df57.columns[0],axis=1)\n",
    "df58 = df58.drop(df58.columns[0],axis=1)\n",
    "df59 = df59.drop(df59.columns[0],axis=1)\n",
    "df60 = df60.drop(df60.columns[0],axis=1)\n",
    "df61 = df61.drop(df61.columns[0],axis=1)\n",
    "df62 = df62.drop(df62.columns[0],axis=1)\n",
    "df63 = df63.drop(df63.columns[0],axis=1)\n",
    "df64 = df64.drop(df64.columns[0],axis=1)\n",
    "df65 = df65.drop(df65.columns[0],axis=1)\n",
    "df66 = df66.drop(df66.columns[0],axis=1)\n",
    "df67 = df67.drop(df67.columns[0],axis=1)\n",
    "df68 = df68.drop(df68.columns[0],axis=1)\n",
    "df69 = df69.drop(df69.columns[0],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_list = [df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17,df18,df19,df20,df21,df22,df23,df24,df25,df26,df27,df28,df29,df30,df31,df32,df33,df34,df35,df36,df37,df38,df39,df40,df41,df42,df43,df44,df45,df46,df47,df48,df49,df50,df51,df52,df53,df54,df55,df56,df57,df58,df59,df60,df61,df62,df63,df64,df65,df66,df67,df68,df69]\n",
    "\n",
    "out =     [0,  0,  0,  0,  0,  0,  0,  0,  0,  1,   1,    1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   2,   2,   2,   2,   2,  2,     2,    2,   2,   2,   2,  2,  3,   3,    3,    3,   3,  3,   3,   3,   3,   3,   3,   3,   4,    4,   4,  4,    4,   4,   4,   4,  4,   4,   4,   4,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5]\n",
    "\n",
    "# 69 datasets, each dataset will me split, features extracted, and instantiated in that same index in a corresponding manner\n",
    "# thus out has 69 entries, corresponding to class in each dataset\n",
    "# 0 : bending (1 & 2)\n",
    "# 1 : cycling\n",
    "# 2 : lying\n",
    "# 3 : sitting\n",
    "# 4 : standing\n",
    "# 5 : walking\n",
    "# ordered as can be seen again their dataframes, corresponding to df_list which will be instantiated in similar index orders\n",
    "# thus this is a multiclass, 6 class problem, total 6 classes from 0 to 5\n",
    "\n",
    "y = pd.DataFrame(out)\n",
    "\n",
    "\n",
    "\n",
    "l = np.arange(1,21)\n",
    "datalist = np.arange(0,69)\n",
    "l_new = [1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15,16,16,17,17,18,18,19,19,20,20]\n",
    "errors_test_estimated = pd.DataFrame(index=l, columns = ['Estimated Test Error'])\n",
    "\n",
    "\n",
    "for i in l:\n",
    "    \n",
    "    splits = np.arange(0,i)\n",
    "    segment = math.floor(480/i)\n",
    "    df_list_new = []\n",
    "    j = 0\n",
    "    for j in datalist:\n",
    "        list_df_specific = []\n",
    "        df_parts = 0\n",
    "        for df_parts in splits:\n",
    "             list_df_specific.append(df_list[j].iloc[((df_parts)*(segment)) : ((df_parts + 1)*(segment)),:].reset_index(drop=True))\n",
    "        df_list_new.append(pd.concat(list_df_specific, axis=1))\n",
    "    \n",
    "    header_column = np.arange(0, 18*i) \n",
    "    features = pd.DataFrame(index=datalist,columns=header_column) \n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "    t=0\n",
    "    for t in datalist:\n",
    "        features.iloc[t, 0 : (6*i)]       =    (((pd.DataFrame(df_list_new[t].mean()).transpose()).to_numpy()).flatten())[:]\n",
    "        features.iloc[t, (6*i) : (12*i)]  =    (((pd.DataFrame(df_list_new[t].std()).transpose()).to_numpy()).flatten())[:]\n",
    "        features.iloc[t, (12*i) : (18*i)] =    (((pd.DataFrame(df_list_new[t].max()).transpose()).to_numpy()).flatten())[:]\n",
    "    \n",
    "    \n",
    "    total = features.copy()\n",
    "    total['Output'] = out\n",
    "    \n",
    "    # note this out will stay same, irrespective of split, as each spit in each dataset in markedly instantiated against\n",
    "    # its corresponding out\n",
    "    # thus no matter the split index row 0 will have instantiated dataset df1 of bending 1 which against it will contain its\n",
    "    # class 0, also in corresponding index row 0\n",
    "    # and so on and so forth for all the corresponding classes in general\n",
    "    \n",
    "    # total has split features with output class, this is only the training data\n",
    "    \n",
    "    # total has the relevant split data that has to be cross validated upon\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True)  # performing 5-fold CV \n",
    "    \n",
    "    X_train = total.drop(columns=['Output'])\n",
    "    y_train = total['Output']                   \n",
    "    \n",
    "    \n",
    "    # the errors will be saved in the error_test_estimate vector\n",
    "    # here external cv is stratified, as it wasn't used before we'll use it here\n",
    "    # additionally internal cv is automatically stratified by sklearn\n",
    "    \n",
    "    cv_error = np.zeros(5) #to store cv errors\n",
    "    \n",
    "    \n",
    "    count_ext = 0\n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        X_train_cv,X_test_cv = X_train.iloc[train_index,:],X_train.iloc[test_index,:]\n",
    "        y_train_cv,y_test_cv = y_train.iloc[train_index],y_train.iloc[test_index]\n",
    "        count_ext = count_ext + 1\n",
    "        log_reg = LogisticRegressionCV(cv=5,solver='liblinear',penalty='l1').fit(X_train_cv, y_train_cv)\n",
    "        \n",
    "        # 5-fold CV picks best lambda/alpha/C \n",
    "        \n",
    "        # note liblinear and sklearn penalizes beta0 also, by default, so it will be considered a 'parameter'\n",
    "\n",
    "        y_test_cv_predicted = log_reg.predict(X_test_cv)\n",
    "        \n",
    "        mis = 0\n",
    "        h=0\n",
    "        for h in np.arange(0,y_test_cv.shape[0]):\n",
    "            if y_test_cv_predicted[h] != y_test_cv.iloc[h]:\n",
    "                mis = mis + 1\n",
    "        \n",
    "        error = (mis/(y_test_cv.shape[0]))*100\n",
    "        \n",
    "        cv_error[count_ext-1]=error\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "   \n",
    "    error_l = np.mean(cv_error)\n",
    "    errors_test_estimated.iloc[i-1,0]=error_l\n",
    "    \n",
    "    \n",
    "    \n",
    "print('The test error estimates : \\n',errors_test_estimated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The l split selected that minimizes the test error estimate for this multiclass classification is : \n",
      " 3\n"
     ]
    }
   ],
   "source": [
    "index = np.argmin(errors_test_estimated)\n",
    "l_selected = index + 1\n",
    "print('The l split selected that minimizes the test error estimate for this multiclass classification is : \\n',l_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments :\n",
    "As can be seen, the total errors are much higher than for binary classification. This could be expected since the overall instances for each class now has reduced. \n",
    "NOTE : Stratified cross validation was used here as we needed to preserve the existence of each class label in each split, otherwise error estimated would be highly overestimated as otherwise certain classes would appear in train and would be there in test\n",
    "\n",
    "Thus stratified Kfold utilization is of utmost importance here in general\n",
    "l=3 has minimum test error estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR actual TEST ERROR :\n",
    "Assumption : Since each run may yield a different l-value as kfold is completely randomized and uses shuffled data, the l value in each run may be different. In general, it should vary mostly between 1-3 (1,2,3 splits) as that consistently across all problems minimized the test error (estimate obviously, via Kfold)\n",
    "\n",
    "Here we have two routes :\n",
    "1. Take the selected l and split according to that l so that variation in l across run time (minor) will ensure that test data is split according to that selected l\n",
    "2. Sectionalize and assume that l=3 is the answer, thus if at a different run time we get l=1, we'll still split data according to current run time result, that is in 3 parts\n",
    "\n",
    "While coding approach 1. is more general, it is again mildly time consuming and repetitive in terms of re-splitting according to the selected l. Obviously 1. isn't that difficult or doesn't need additonal coding or more code blocks, just a code block that splits everytime according to the obtained l\n",
    "In general, we'll take approach 2 into consideration here and go forth with that, as opposed to approach 1.\n",
    "\n",
    "THUS\n",
    "NOTE : Runtime change may give different l, we'll assume l=3 is sufficiently good estimate for l over large number of trials\n",
    "\n",
    "Thus we will split test into 3 parts, and obtain the test error on it by training it on train data, that we will also split once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1        2         3        4         5        6  \\\n",
      "0   43.9451  0.350125  23.3911     0.455  36.3489  0.530687  44.3242   \n",
      "1   43.6212   0.59475  23.1458   0.71375  36.5305  0.654437  42.1004   \n",
      "2   40.6738  0.698312  18.8588   1.25144  29.0874  0.439375  42.8464   \n",
      "3   44.7921  0.177937  18.9189   1.11831  24.0943    0.8115  42.8199   \n",
      "4   43.0673   0.40875  15.4131  0.825563  23.4773  0.522188  44.6137   \n",
      "..      ...       ...      ...       ...      ...       ...      ...   \n",
      "64  33.3256   4.37131   15.106   3.25338  15.8803   3.23538  33.6204   \n",
      "65  34.1833   4.85762  15.0886   3.26744  15.4443   3.42537  34.2593   \n",
      "66  34.0153   4.62231  15.5089   3.12131  16.1948   3.29656  34.5384   \n",
      "67  34.8948    4.4205  15.5336   3.12975  15.6431   3.44369  34.8862   \n",
      "68  35.0022   4.28537  15.1794   3.20906  15.6646   3.29419  34.2569   \n",
      "\n",
      "           7        8         9  ...    45     46    47     48     49     50  \\\n",
      "0   0.408062  22.3036     0.623  ...  4.44  38.33  1.79   47.4    1.7  29.75   \n",
      "1    0.72825  21.7769   1.06781  ...  4.77   38.5  1.53  45.25   2.86  29.25   \n",
      "2     0.4405  19.7239  0.713938  ...  5.55   37.5  1.79   42.5   1.12     24   \n",
      "3     0.4435  14.8452      0.88  ...   4.5  26.25  2.87   46.5   1.58  23.25   \n",
      "4   0.371437  16.7839  0.460625  ...   2.5   25.5  2.12     48   1.25     22   \n",
      "..       ...      ...       ...  ...   ...    ...   ...    ...    ...    ...   \n",
      "64   4.60513  15.5265   3.40375  ...     9     23  8.32  45.33  14.67  23.25   \n",
      "65   4.33025  15.2655   3.46819  ...  6.76  23.25  8.18  43.75   12.5  22.25   \n",
      "66   4.25163  15.1973   2.66706  ...  7.04  22.75  7.32     46  12.28     21   \n",
      "67   4.24369   15.632   3.40763  ...   9.9  22.67  9.51  45.33  11.02  22.67   \n",
      "68   4.27631  15.5065    3.2925  ...   9.1  23.25     9  43.33   10.2  22.75   \n",
      "\n",
      "      51     52    53 Output  \n",
      "0   2.05  38.25   1.5      0  \n",
      "1   5.15  38.25  2.18      0  \n",
      "2   2.35   30.5     1      0  \n",
      "3   5.21     26  4.06      0  \n",
      "4   3.56     26  2.96      0  \n",
      "..   ...    ...   ...    ...  \n",
      "64   7.4  23.33  6.83      5  \n",
      "65  7.46   21.5  9.67      5  \n",
      "66  8.22     21  8.64      5  \n",
      "67     7   23.5  7.31      5  \n",
      "68  7.65  22.33  8.26      5  \n",
      "\n",
      "[69 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "i=3       # train split into 3\n",
    "    \n",
    "splits = np.arange(0,i)\n",
    "segment = math.floor(480/i)\n",
    "df_list_new = []\n",
    "j = 0\n",
    "for j in datalist:\n",
    "    list_df_specific = []\n",
    "    df_parts = 0\n",
    "    for df_parts in splits:\n",
    "         list_df_specific.append(df_list[j].iloc[((df_parts)*(segment)) : ((df_parts + 1)*(segment)),:].reset_index(drop=True))\n",
    "    df_list_new.append(pd.concat(list_df_specific, axis=1))\n",
    "\n",
    "header_column = np.arange(0, 18*i) \n",
    "features = pd.DataFrame(index=datalist,columns=header_column) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "t=0\n",
    "for t in datalist:\n",
    "    features.iloc[t, 0 : (6*i)]       =    (((pd.DataFrame(df_list_new[t].mean()).transpose()).to_numpy()).flatten())[:]\n",
    "    features.iloc[t, (6*i) : (12*i)]  =    (((pd.DataFrame(df_list_new[t].std()).transpose()).to_numpy()).flatten())[:]\n",
    "    features.iloc[t, (12*i) : (18*i)] =    (((pd.DataFrame(df_list_new[t].max()).transpose()).to_numpy()).flatten())[:]\n",
    "    \n",
    "total = features.copy()\n",
    "total['Output'] = out\n",
    "\n",
    "print(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1        2         3        4         5        6   \\\n",
      "0   43.9451  0.350125  23.3911     0.455  36.3489  0.530687  44.3242   \n",
      "1   43.6212   0.59475  23.1458   0.71375  36.5305  0.654437  42.1004   \n",
      "2   40.6738  0.698312  18.8588   1.25144  29.0874  0.439375  42.8464   \n",
      "3   44.7921  0.177937  18.9189   1.11831  24.0943    0.8115  42.8199   \n",
      "4   43.0673   0.40875  15.4131  0.825563  23.4773  0.522188  44.6137   \n",
      "..      ...       ...      ...       ...      ...       ...      ...   \n",
      "64  33.3256   4.37131   15.106   3.25338  15.8803   3.23538  33.6204   \n",
      "65  34.1833   4.85762  15.0886   3.26744  15.4443   3.42537  34.2593   \n",
      "66  34.0153   4.62231  15.5089   3.12131  16.1948   3.29656  34.5384   \n",
      "67  34.8948    4.4205  15.5336   3.12975  15.6431   3.44369  34.8862   \n",
      "68  35.0022   4.28537  15.1794   3.20906  15.6646   3.29419  34.2569   \n",
      "\n",
      "          7        8         9   ...     44    45     46    47     48     49  \\\n",
      "0   0.408062  22.3036     0.623  ...  29.25  4.44  38.33  1.79   47.4    1.7   \n",
      "1    0.72825  21.7769   1.06781  ...  28.25  4.77   38.5  1.53  45.25   2.86   \n",
      "2     0.4405  19.7239  0.713938  ...  28.25  5.55   37.5  1.79   42.5   1.12   \n",
      "3     0.4435  14.8452      0.88  ...  20.75   4.5  26.25  2.87   46.5   1.58   \n",
      "4   0.371437  16.7839  0.460625  ...     24   2.5   25.5  2.12     48   1.25   \n",
      "..       ...      ...       ...  ...    ...   ...    ...   ...    ...    ...   \n",
      "64   4.60513  15.5265   3.40375  ...  22.75     9     23  8.32  45.33  14.67   \n",
      "65   4.33025  15.2655   3.46819  ...     22  6.76  23.25  8.18  43.75   12.5   \n",
      "66   4.25163  15.1973   2.66706  ...   22.5  7.04  22.75  7.32     46  12.28   \n",
      "67   4.24369   15.632   3.40763  ...  23.25   9.9  22.67  9.51  45.33  11.02   \n",
      "68   4.27631  15.5065    3.2925  ...     21   9.1  23.25     9  43.33   10.2   \n",
      "\n",
      "       50    51     52    53  \n",
      "0   29.75  2.05  38.25   1.5  \n",
      "1   29.25  5.15  38.25  2.18  \n",
      "2      24  2.35   30.5     1  \n",
      "3   23.25  5.21     26  4.06  \n",
      "4      22  3.56     26  2.96  \n",
      "..    ...   ...    ...   ...  \n",
      "64  23.25   7.4  23.33  6.83  \n",
      "65  22.25  7.46   21.5  9.67  \n",
      "66     21  8.22     21  8.64  \n",
      "67  22.67     7   23.5  7.31  \n",
      "68  22.75  7.65  22.33  8.26  \n",
      "\n",
      "[69 rows x 54 columns]\n",
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "     ..\n",
      "64    5\n",
      "65    5\n",
      "66    5\n",
      "67    5\n",
      "68    5\n",
      "Name: Output, Length: 69, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train = total.drop(columns = ['Output'])\n",
    "y_train = total['Output']\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1        2         3        4         5        6  \\\n",
      "0   41.3626  0.256125  18.3271  0.581188  34.0511    0.6515  39.6206   \n",
      "1   42.9217  0.468438  19.9269  0.834688  33.4829  0.505687  41.7014   \n",
      "2   22.6099  0.871188   19.152  0.603438  25.9284  0.536312  25.7496   \n",
      "3   28.7929  0.211625  19.5479  0.613125  17.3392  0.863625  28.6321   \n",
      "4   36.7948   2.45981  17.5693   3.06381  19.1185   2.95456  37.3584   \n",
      "5   37.5218    2.1565  16.0797   3.28169  18.7794    2.9165  37.5402   \n",
      "6   38.0289   2.00513  16.3569   2.69894  19.7927   2.49438  37.0514   \n",
      "7   27.7881  0.234312  6.04856     0.846    9.313  0.607875   26.624   \n",
      "8   36.5416  0.291875  6.68925  0.904063  7.79194  0.960563  48.0072   \n",
      "9        48         0  5.61125  0.397187  3.78381  0.696625  48.0016   \n",
      "10  42.3121   0.29425  16.3537  0.729625  9.86094   1.26156  43.4542   \n",
      "11  45.9493  0.213687  13.6939  0.787188  18.2038  0.801688  46.0373   \n",
      "12  45.5889  0.349562  14.6733   1.13175  18.8676   1.08931  45.1317   \n",
      "13  46.2424   0.43575  11.4991   1.01881  17.5445   1.02244  44.7127   \n",
      "14  42.3467   0.54375  13.8994   0.55125  14.1241  0.853063  43.8959   \n",
      "15  44.5953  0.435312  13.9743  0.638688  16.3777  0.792063  44.4996   \n",
      "16  34.5614   4.01756  15.8817   3.08044  16.4264    3.5465  33.7593   \n",
      "17  33.6665   4.54725  15.6154   3.27044  16.0994   3.56138  33.6665   \n",
      "18  33.3646     4.637  15.4641   3.16181  16.0541   3.37025  34.4727   \n",
      "\n",
      "            7        8         9  ...    45     46    47     48     49     50  \\\n",
      "0    0.501562  17.7877  0.910125  ...  4.55  37.75  1.92     45    1.3   29.5   \n",
      "1    0.251437  20.4998  0.716813  ...  5.76   38.5  3.11  45.67   1.12  26.75   \n",
      "2    0.604063  18.9608  0.804313  ...     6     30  4.74   27.5   0.94     24   \n",
      "3    0.309125  19.7214   0.61175  ...  2.49     24  3.86  42.75   7.76     35   \n",
      "4     2.36131  16.0188   3.04519  ...  7.83   25.5  9.07  44.67   8.58     23   \n",
      "5       2.066  16.8609   2.98988  ...  8.32     24  9.62     44   9.91  24.67   \n",
      "6     2.46381  16.8747    3.2235  ...  7.36  23.75  8.55     44  14.17     24   \n",
      "7    0.430375  6.78113  0.962562  ...  3.42     21   4.5     30   1.25     13   \n",
      "8     0.01375  7.50044  0.556375  ...   3.2     10  3.77     48      0    9.5   \n",
      "9   0.0026875   4.2785     0.384  ...  2.45     12   2.5  48.25   0.43   9.75   \n",
      "10   0.515812  17.9449  0.392562  ...  1.87     21  2.59     48   4.44   22.5   \n",
      "11   0.332562  17.9924  0.547375  ...  5.05     24  4.95  46.67      1  22.25   \n",
      "12     0.1635  11.9635    1.2465  ...  4.55  23.75  2.29  45.25    0.5  20.75   \n",
      "13   0.495812  9.46856   1.03213  ...  5.54     23  5.02     45    1.3  18.25   \n",
      "14   0.475187  11.7131  0.942563  ...  6.56     18  5.72  46.25   1.79     18   \n",
      "15   0.449062  15.9906     0.535  ...  3.86  21.33  3.74     47   3.34     21   \n",
      "16    3.88694  15.6397   3.60825  ...  8.73     26  7.43  42.75   9.68   23.5   \n",
      "17    4.25381  15.3906   3.39088  ...   9.1  23.75  8.96  43.25  12.85  23.75   \n",
      "18    3.96175   15.428   3.11231  ...  8.58   24.5  7.59  46.75  13.44     21   \n",
      "\n",
      "      51     52    53 Output  \n",
      "0    5.5  38.25  1.87      0  \n",
      "1   5.72  36.67  2.62      0  \n",
      "2   6.76     24  4.97      0  \n",
      "3   4.76     33  6.76      0  \n",
      "4   7.43  24.33  9.34      1  \n",
      "5   7.69  24.33  8.83      1  \n",
      "6   9.74  21.33   8.5      1  \n",
      "7   5.02     17   3.3      2  \n",
      "8   0.94    4.5   1.5      2  \n",
      "9   2.12     12   1.3      2  \n",
      "10  5.36     28  5.85      3  \n",
      "11  4.58     22  7.22      3  \n",
      "12  6.36     22  3.49      3  \n",
      "13  4.76  18.25  3.27      4  \n",
      "14     3  21.25  1.87      4  \n",
      "15  5.85     21  5.73      4  \n",
      "16  8.86  25.25  8.86      5  \n",
      "17   7.4  24.25  8.75      5  \n",
      "18  7.22     24  8.26      5  \n",
      "\n",
      "[19 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "df_n_1 = pd.read_csv('bending1\\\\test\\\\dataset1.csv')\n",
    "df_n_2 = pd.read_csv('bending1\\\\test\\\\dataset2.csv')\n",
    "\n",
    "df_n_3 = pd.read_csv('bending2\\\\test\\\\dataset1.csv')\n",
    "df_n_4= pd.read_csv('bending2\\\\test\\\\dataset2.csv')\n",
    "\n",
    "df_n_5 = pd.read_csv('cycling\\\\test\\\\dataset1.csv')\n",
    "df_n_6 = pd.read_csv('cycling\\\\test\\\\dataset2.csv')\n",
    "df_n_7 = pd.read_csv('cycling\\\\test\\\\dataset3.csv')\n",
    "\n",
    "df_n_8 = pd.read_csv('lying\\\\test\\\\dataset1.csv')\n",
    "df_n_9 = pd.read_csv('lying\\\\test\\\\dataset2.csv')\n",
    "df_n_10 = pd.read_csv('lying\\\\test\\\\dataset3.csv')\n",
    "\n",
    "df_n_11 = pd.read_csv('sitting\\\\test\\\\dataset1.csv')\n",
    "df_n_12 = pd.read_csv('sitting\\\\test\\\\dataset2.csv')\n",
    "df_n_13 = pd.read_csv('sitting\\\\test\\\\dataset3.csv')\n",
    "\n",
    "df_n_14 = pd.read_csv('standing\\\\test\\\\dataset1.csv')\n",
    "df_n_15 = pd.read_csv('standing\\\\test\\\\dataset2.csv')\n",
    "df_n_16 = pd.read_csv('standing\\\\test\\\\dataset3.csv')\n",
    "\n",
    "df_n_17 = pd.read_csv('walking\\\\test\\\\dataset1.csv')\n",
    "df_n_18 = pd.read_csv('walking\\\\test\\\\dataset2.csv')\n",
    "df_n_19 = pd.read_csv('walking\\\\test\\\\dataset3.csv')\n",
    "\n",
    "df_n_1 = df_n_1.drop(df_n_1.columns[0],axis=1)\n",
    "df_n_2 = df_n_2.drop(df_n_2.columns[0],axis=1)\n",
    "df_n_3 = df_n_3.drop(df_n_3.columns[0],axis=1)\n",
    "df_n_4 = df_n_4.drop(df_n_4.columns[0],axis=1)\n",
    "df_n_5 = df_n_5.drop(df_n_5.columns[0],axis=1)\n",
    "df_n_6 = df_n_6.drop(df_n_6.columns[0],axis=1)\n",
    "df_n_7 = df_n_7.drop(df_n_7.columns[0],axis=1)\n",
    "df_n_8 = df_n_8.drop(df_n_8.columns[0],axis=1)\n",
    "df_n_9 = df_n_9.drop(df_n_9.columns[0],axis=1)\n",
    "df_n_10 = df_n_10.drop(df_n_10.columns[0],axis=1)\n",
    "df_n_11 = df_n_11.drop(df_n_11.columns[0],axis=1)\n",
    "df_n_12 = df_n_12.drop(df_n_12.columns[0],axis=1)\n",
    "df_n_13 = df_n_13.drop(df_n_13.columns[0],axis=1)\n",
    "df_n_14 = df_n_14.drop(df_n_14.columns[0],axis=1)\n",
    "df_n_15 = df_n_15.drop(df_n_15.columns[0],axis=1)\n",
    "df_n_16 = df_n_16.drop(df_n_16.columns[0],axis=1)\n",
    "df_n_17 = df_n_17.drop(df_n_17.columns[0],axis=1)\n",
    "df_n_18 = df_n_18.drop(df_n_18.columns[0],axis=1)\n",
    "df_n_19 = df_n_19.drop(df_n_19.columns[0],axis=1)\n",
    "\n",
    "df_list_test = [df_n_1,df_n_2,df_n_3,df_n_4,df_n_5,df_n_6,df_n_7,df_n_8,df_n_9,df_n_10,df_n_11,df_n_12,df_n_13,df_n_14,df_n_15,df_n_16,df_n_17,df_n_18,df_n_19]\n",
    "\n",
    "out_test =     [0,      0,      0,     0,     1,     1,     1,     2,     2,     2,     3,        3,      3,     4,      4,     4,       5,      5,       5]\n",
    "\n",
    "y_test = pd.DataFrame(out_test)\n",
    "\n",
    "datalist_test = np.arange(0,19)\n",
    "\n",
    "\n",
    "\n",
    "i=3       # test split into 3\n",
    "    \n",
    "splits = np.arange(0,i)\n",
    "segment = math.floor(480/i)\n",
    "df_list_new = []\n",
    "j = 0\n",
    "for j in datalist_test:\n",
    "    list_df_specific = []\n",
    "    df_parts = 0\n",
    "    for df_parts in splits:\n",
    "         list_df_specific.append(df_list_test[j].iloc[((df_parts)*(segment)) : ((df_parts + 1)*(segment)),:].reset_index(drop=True))\n",
    "    df_list_new.append(pd.concat(list_df_specific, axis=1))\n",
    "\n",
    "header_column = np.arange(0, 18*i) \n",
    "features_test = pd.DataFrame(index=datalist_test,columns=header_column) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "t=0\n",
    "for t in datalist_test:\n",
    "    features_test.iloc[t, 0 : (6*i)]       =    (((pd.DataFrame(df_list_new[t].mean()).transpose()).to_numpy()).flatten())[:]\n",
    "    features_test.iloc[t, (6*i) : (12*i)]  =    (((pd.DataFrame(df_list_new[t].std()).transpose()).to_numpy()).flatten())[:]\n",
    "    features_test.iloc[t, (12*i) : (18*i)] =    (((pd.DataFrame(df_list_new[t].max()).transpose()).to_numpy()).flatten())[:]\n",
    "    \n",
    "total_test = features_test.copy()\n",
    "total_test['Output'] = out_test\n",
    "\n",
    "print(total_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1        2         3        4         5        6   \\\n",
      "0   41.3626  0.256125  18.3271  0.581188  34.0511    0.6515  39.6206   \n",
      "1   42.9217  0.468438  19.9269  0.834688  33.4829  0.505687  41.7014   \n",
      "2   22.6099  0.871188   19.152  0.603438  25.9284  0.536312  25.7496   \n",
      "3   28.7929  0.211625  19.5479  0.613125  17.3392  0.863625  28.6321   \n",
      "4   36.7948   2.45981  17.5693   3.06381  19.1185   2.95456  37.3584   \n",
      "5   37.5218    2.1565  16.0797   3.28169  18.7794    2.9165  37.5402   \n",
      "6   38.0289   2.00513  16.3569   2.69894  19.7927   2.49438  37.0514   \n",
      "7   27.7881  0.234312  6.04856     0.846    9.313  0.607875   26.624   \n",
      "8   36.5416  0.291875  6.68925  0.904063  7.79194  0.960563  48.0072   \n",
      "9        48         0  5.61125  0.397187  3.78381  0.696625  48.0016   \n",
      "10  42.3121   0.29425  16.3537  0.729625  9.86094   1.26156  43.4542   \n",
      "11  45.9493  0.213687  13.6939  0.787188  18.2038  0.801688  46.0373   \n",
      "12  45.5889  0.349562  14.6733   1.13175  18.8676   1.08931  45.1317   \n",
      "13  46.2424   0.43575  11.4991   1.01881  17.5445   1.02244  44.7127   \n",
      "14  42.3467   0.54375  13.8994   0.55125  14.1241  0.853063  43.8959   \n",
      "15  44.5953  0.435312  13.9743  0.638688  16.3777  0.792063  44.4996   \n",
      "16  34.5614   4.01756  15.8817   3.08044  16.4264    3.5465  33.7593   \n",
      "17  33.6665   4.54725  15.6154   3.27044  16.0994   3.56138  33.6665   \n",
      "18  33.3646     4.637  15.4641   3.16181  16.0541   3.37025  34.4727   \n",
      "\n",
      "           7        8         9   ...     44    45     46    47     48     49  \\\n",
      "0    0.501562  17.7877  0.910125  ...  27.25  4.55  37.75  1.92     45    1.3   \n",
      "1    0.251437  20.4998  0.716813  ...   29.5  5.76   38.5  3.11  45.67   1.12   \n",
      "2    0.604063  18.9608  0.804313  ...  25.33     6     30  4.74   27.5   0.94   \n",
      "3    0.309125  19.7214   0.61175  ...     28  2.49     24  3.86  42.75   7.76   \n",
      "4     2.36131  16.0188   3.04519  ...  23.75  7.83   25.5  9.07  44.67   8.58   \n",
      "5       2.066  16.8609   2.98988  ...     24  8.32     24  9.62     44   9.91   \n",
      "6     2.46381  16.8747    3.2235  ...     23  7.36  23.75  8.55     44  14.17   \n",
      "7    0.430375  6.78113  0.962562  ...  13.25  3.42     21   4.5     30   1.25   \n",
      "8     0.01375  7.50044  0.556375  ...     11   3.2     10  3.77     48      0   \n",
      "9   0.0026875   4.2785     0.384  ...     13  2.45     12   2.5  48.25   0.43   \n",
      "10   0.515812  17.9449  0.392562  ...   22.5  1.87     21  2.59     48   4.44   \n",
      "11   0.332562  17.9924  0.547375  ...   21.5  5.05     24  4.95  46.67      1   \n",
      "12     0.1635  11.9635    1.2465  ...  21.25  4.55  23.75  2.29  45.25    0.5   \n",
      "13   0.495812  9.46856   1.03213  ...  17.33  5.54     23  5.02     45    1.3   \n",
      "14   0.475187  11.7131  0.942563  ...  20.67  6.56     18  5.72  46.25   1.79   \n",
      "15   0.449062  15.9906     0.535  ...  20.67  3.86  21.33  3.74     47   3.34   \n",
      "16    3.88694  15.6397   3.60825  ...  21.67  8.73     26  7.43  42.75   9.68   \n",
      "17    4.25381  15.3906   3.39088  ...     23   9.1  23.75  8.96  43.25  12.85   \n",
      "18    3.96175   15.428   3.11231  ...  25.25  8.58   24.5  7.59  46.75  13.44   \n",
      "\n",
      "       50    51     52    53  \n",
      "0    29.5   5.5  38.25  1.87  \n",
      "1   26.75  5.72  36.67  2.62  \n",
      "2      24  6.76     24  4.97  \n",
      "3      35  4.76     33  6.76  \n",
      "4      23  7.43  24.33  9.34  \n",
      "5   24.67  7.69  24.33  8.83  \n",
      "6      24  9.74  21.33   8.5  \n",
      "7      13  5.02     17   3.3  \n",
      "8     9.5  0.94    4.5   1.5  \n",
      "9    9.75  2.12     12   1.3  \n",
      "10   22.5  5.36     28  5.85  \n",
      "11  22.25  4.58     22  7.22  \n",
      "12  20.75  6.36     22  3.49  \n",
      "13  18.25  4.76  18.25  3.27  \n",
      "14     18     3  21.25  1.87  \n",
      "15     21  5.85     21  5.73  \n",
      "16   23.5  8.86  25.25  8.86  \n",
      "17  23.75   7.4  24.25  8.75  \n",
      "18     21  7.22     24  8.26  \n",
      "\n",
      "[19 rows x 54 columns]\n",
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     1\n",
      "5     1\n",
      "6     1\n",
      "7     2\n",
      "8     2\n",
      "9     2\n",
      "10    3\n",
      "11    3\n",
      "12    3\n",
      "13    4\n",
      "14    4\n",
      "15    4\n",
      "16    5\n",
      "17    5\n",
      "18    5\n",
      "Name: Output, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_test = total_test.drop(columns=['Output'])\n",
    "y_test = total_test['Output']\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     1\n",
      "5     1\n",
      "6     1\n",
      "7     2\n",
      "8     2\n",
      "9     2\n",
      "10    3\n",
      "11    3\n",
      "12    3\n",
      "13    4\n",
      "14    4\n",
      "15    4\n",
      "16    5\n",
      "17    5\n",
      "18    5\n",
      "Name: Output, dtype: int64\n",
      "[0 0 0 4 1 1 5 2 4 2 4 3 3 2 2 4 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "cls = LogisticRegressionCV(cv=5,solver='liblinear',penalty='l1').fit(X_train, y_train)\n",
    "y_test_predicted = cls.predict(X_test)    \n",
    "print(y_test)\n",
    "print(y_test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual test error is : \n",
      " 31.57894736842105 %\n"
     ]
    }
   ],
   "source": [
    "wrng = 0\n",
    "for q in np.arange(0,y_test.shape[0]):\n",
    "    if y_test_predicted[q] != y_test.iloc[q]:\n",
    "        wrng = wrng + 1\n",
    "\n",
    "wrngper = (wrng/(y_test.shape[0]))*100\n",
    "print('The actual test error is : \\n',wrngper,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, despite being trained on the entire training data in the current case, the actual test error obtained was 31 %, much higher than estimated test error 11 %\n",
    "\n",
    "For my Run :\n",
    "Best Split, l = 3\n",
    "Test Error = 31 %\n",
    "Estimated Test Error = 11 %\n",
    "\n",
    "Based on runs l could change, and thus estimated error for test corresponding to an l could change. This is probably why in the current case, the test error is so different from its estimate\n",
    "\n",
    "One of the reasons why this happens here more so is because despite having 6 classes, the dataset contains only 69 instances to train the dataset and the test dataset is comparable in size to the train\n",
    "\n",
    "Thus we have such fluidity in test errors, their estimates, and obtained optimal l, but more often than not l will fall in between 1,2,3 splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I don't have R, I can't use pROC\n",
    "I've done ROC manually for the previous portions, but this would clearly be time consuming here\n",
    "Thus, I've chosen to not perform the :\n",
    "Confusion Matrix, ROC, AUC for this multi-class classification\n",
    "As such we could use either the macro or the micro techniques of estimating them as discussed in the lectures, and use manual methods rather than pROC to do these steps. It shouldn't be difficult or that different, coding wise, just time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
