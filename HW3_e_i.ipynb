{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW3\n",
    "e\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix of features and their coefficients for different l is : \n",
      "    0          1   2         3   4         5   6   7   8          9    ...  \\\n",
      "1    0          1   2         3   4         5   6   7   8          9  ...   \n",
      "1    0   -0.59713   0 -0.739137   0   1.16691   0   0   0  -0.191012  ...   \n",
      "2    0          1   2         3   4         5   6   7   8          9  ...   \n",
      "2    0 -0.0738821   0 -0.129993   0  0.183875   0   0   0          0  ...   \n",
      "3    0          1   2         3   4         5   6   7   8          9  ...   \n",
      "3    0          0   0 -0.107597   0  0.282708   0   0   0          0  ...   \n",
      "4    0          1   2         3   4         5   6   7   8          9  ...   \n",
      "4    0          0   0 -0.184273   0  0.444063   0   0   0          0  ...   \n",
      "5    0          1   2         3   4         5   6   7   8          9  ...   \n",
      "5    0          0   0 -0.150842   0  0.420312   0   0   0          0  ...   \n",
      "6    0          1   2         3   4         5   6   7   8          9  ...   \n",
      "6    0          0   0         0   0  0.212108   0   0   0          0  ...   \n",
      "7    0          1   2         3   4         5   6   7   8          9  ...   \n",
      "7    0          0   0 -0.174397   0  0.141331   0   0   0          0  ...   \n",
      "8    0          1   2         3   4         5   6   7   8          9  ...   \n",
      "8    0          0   0 -0.108887   0         0   0   0   0  -0.072811  ...   \n",
      "9    0          1   2         3   4         5   6   7   8          9  ...   \n",
      "9    0          0   0         0   0         0   0   0   0  -0.131666  ...   \n",
      "10   0          1   2         3   4         5   6   7   8          9  ...   \n",
      "10   0          0   0         0   0         0   0   0   0  -0.338488  ...   \n",
      "11   0          1   2         3   4         5   6   7   8          9  ...   \n",
      "11   0          0   0         0   0         0   0   0   0          0  ...   \n",
      "12   0          1   2         3   4         5   6   7   8          9  ...   \n",
      "12   0          0   0         0   0         0   0   0   0          0  ...   \n",
      "13   0          1   2         3   4         5   6   7   8          9  ...   \n",
      "13   0          0   0         0   0         0   0   0   0 -0.0908401  ...   \n",
      "14   0          1   2         3   4         5   6   7   8          9  ...   \n",
      "14   0          0   0         0   0         0   0   0   0          0  ...   \n",
      "15   0          1   2         3   4         5   6   7   8          9  ...   \n",
      "15   0          0   0         0   0         0   0   0   0          0  ...   \n",
      "16   0          1   2         3   4         5   6   7   8          9  ...   \n",
      "16   0          0   0         0   0         0   0   0   0          0  ...   \n",
      "17   0          1   2         3   4         5   6   7   8          9  ...   \n",
      "17   0          0   0         0   0         0   0   0   0          0  ...   \n",
      "18   0          1   2         3   4         5   6   7   8          9  ...   \n",
      "18   0          0   0         0   0         0   0   0   0          0  ...   \n",
      "19   0          1   2         3   4         5   6   7   8          9  ...   \n",
      "19   0          0   0         0   0         0   0   0   0          0  ...   \n",
      "20   0          1   2         3   4         5   6   7   8          9  ...   \n",
      "20   0          0   0         0   0         0   0   0   0          0  ...   \n",
      "\n",
      "    351  352  353  354  355  356  357  358  359  360  \n",
      "1   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "1   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "2   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "2   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "3   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "3   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "4   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "4   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "5   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "5   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "6   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "6   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "7   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "7   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "8   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "8   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "9   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "10  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "10  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "11  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "11  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "12  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "12  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "13  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "13  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "14  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "14  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "15  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "15  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "16  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "16  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "17  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "17  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "18  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "18  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "19  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "19  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "20  351  352  353  354  355  356  357  358  359  360  \n",
      "20    0    0    0    0    0    0    0    0    0    0  \n",
      "\n",
      "[40 rows x 361 columns]\n",
      "The test error estimates : \n",
      "    Estimated Test Error\n",
      "1               5.71429\n",
      "2                4.3956\n",
      "3                4.3956\n",
      "4               5.71429\n",
      "5               7.14286\n",
      "6               8.57143\n",
      "7               8.68132\n",
      "8               8.79121\n",
      "9               7.14286\n",
      "10              7.25275\n",
      "11              5.71429\n",
      "12              7.14286\n",
      "13              7.25275\n",
      "14              5.82418\n",
      "15              4.28571\n",
      "16              5.82418\n",
      "17              5.71429\n",
      "18              5.71429\n",
      "19              7.25275\n",
      "20              5.71429\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('bending1\\\\training\\\\dataset3.csv')\n",
    "df2 = pd.read_csv('bending1\\\\training\\\\dataset4.csv')\n",
    "df3 = pd.read_csv('bending1\\\\training\\\\dataset5.csv')\n",
    "df4 = pd.read_csv('bending1\\\\training\\\\dataset6.csv')\n",
    "df5 = pd.read_csv('bending1\\\\training\\\\dataset7.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df6 = pd.read_csv('bending2\\\\training\\\\dataset3.csv')\n",
    "df7 = pd.read_csv('bending2\\\\training\\\\dataset4.csv')\n",
    "df8 = pd.read_csv('bending2\\\\training\\\\dataset5.csv')\n",
    "df9 = pd.read_csv('bending2\\\\training\\\\dataset6.csv')\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "df10 = pd.read_csv('cycling\\\\training\\\\dataset4.csv')\n",
    "df11 = pd.read_csv('cycling\\\\training\\\\dataset5.csv')\n",
    "df12 = pd.read_csv('cycling\\\\training\\\\dataset6.csv')\n",
    "df13 = pd.read_csv('cycling\\\\training\\\\dataset7.csv')\n",
    "df14 = pd.read_csv('cycling\\\\training\\\\dataset8.csv')\n",
    "df15 = pd.read_csv('cycling\\\\training\\\\dataset9.csv')\n",
    "df16 = pd.read_csv('cycling\\\\training\\\\dataset10.csv')\n",
    "df17 = pd.read_csv('cycling\\\\training\\\\dataset11.csv')\n",
    "df18 = pd.read_csv('cycling\\\\training\\\\dataset12.csv')\n",
    "df19 = pd.read_csv('cycling\\\\training\\\\dataset13.csv')\n",
    "df20 = pd.read_csv('cycling\\\\training\\\\dataset14.csv')\n",
    "df21 = pd.read_csv('cycling\\\\training\\\\dataset15.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df22 = pd.read_csv('lying\\\\training\\\\dataset4.csv')\n",
    "df23 = pd.read_csv('lying\\\\training\\\\dataset5.csv')\n",
    "df24 = pd.read_csv('lying\\\\training\\\\dataset6.csv')\n",
    "df25 = pd.read_csv('lying\\\\training\\\\dataset7.csv')\n",
    "df26 = pd.read_csv('lying\\\\training\\\\dataset8.csv')\n",
    "df27 = pd.read_csv('lying\\\\training\\\\dataset9.csv')\n",
    "df28 = pd.read_csv('lying\\\\training\\\\dataset10.csv')\n",
    "df29 = pd.read_csv('lying\\\\training\\\\dataset11.csv')\n",
    "df30 = pd.read_csv('lying\\\\training\\\\dataset12.csv')\n",
    "df31 = pd.read_csv('lying\\\\training\\\\dataset13.csv')\n",
    "df32 = pd.read_csv('lying\\\\training\\\\dataset14.csv')\n",
    "df33 = pd.read_csv('lying\\\\training\\\\dataset15.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "df34 = pd.read_csv('sitting\\\\training\\\\dataset4.csv')\n",
    "df35 = pd.read_csv('sitting\\\\training\\\\dataset5.csv')\n",
    "df36 = pd.read_csv('sitting\\\\training\\\\dataset6.csv')\n",
    "df37 = pd.read_csv('sitting\\\\training\\\\dataset7.csv')\n",
    "df38 = pd.read_csv('sitting\\\\training\\\\dataset8.csv')\n",
    "df39 = pd.read_csv('sitting\\\\training\\\\dataset9.csv')\n",
    "df40 = pd.read_csv('sitting\\\\training\\\\dataset10.csv')\n",
    "df41 = pd.read_csv('sitting\\\\training\\\\dataset11.csv')\n",
    "df42 = pd.read_csv('sitting\\\\training\\\\dataset12.csv')\n",
    "df43 = pd.read_csv('sitting\\\\training\\\\dataset13.csv')\n",
    "df44 = pd.read_csv('sitting\\\\training\\\\dataset14.csv')\n",
    "df45 = pd.read_csv('sitting\\\\training\\\\dataset15.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df46 = pd.read_csv('standing\\\\training\\\\dataset4.csv')\n",
    "df47 = pd.read_csv('standing\\\\training\\\\dataset5.csv')\n",
    "df48 = pd.read_csv('standing\\\\training\\\\dataset6.csv')\n",
    "df49 = pd.read_csv('standing\\\\training\\\\dataset7.csv')\n",
    "df50 = pd.read_csv('standing\\\\training\\\\dataset8.csv')\n",
    "df51 = pd.read_csv('standing\\\\training\\\\dataset9.csv')\n",
    "df52 = pd.read_csv('standing\\\\training\\\\dataset10.csv')\n",
    "df53 = pd.read_csv('standing\\\\training\\\\dataset11.csv')\n",
    "df54 = pd.read_csv('standing\\\\training\\\\dataset12.csv')\n",
    "df55 = pd.read_csv('standing\\\\training\\\\dataset13.csv')\n",
    "df56 = pd.read_csv('standing\\\\training\\\\dataset14.csv')\n",
    "df57 = pd.read_csv('standing\\\\training\\\\dataset15.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df58 = pd.read_csv('walking\\\\training\\\\dataset4.csv')\n",
    "df59 = pd.read_csv('walking\\\\training\\\\dataset5.csv')\n",
    "df60 = pd.read_csv('walking\\\\training\\\\dataset6.csv')\n",
    "df61 = pd.read_csv('walking\\\\training\\\\dataset7.csv')\n",
    "df62 = pd.read_csv('walking\\\\training\\\\dataset8.csv')\n",
    "df63 = pd.read_csv('walking\\\\training\\\\dataset9.csv')\n",
    "df64 = pd.read_csv('walking\\\\training\\\\dataset10.csv')\n",
    "df65 = pd.read_csv('walking\\\\training\\\\dataset11.csv')\n",
    "df66 = pd.read_csv('walking\\\\training\\\\dataset12.csv')\n",
    "df67 = pd.read_csv('walking\\\\training\\\\dataset13.csv')\n",
    "df68 = pd.read_csv('walking\\\\training\\\\dataset14.csv')\n",
    "df69 = pd.read_csv('walking\\\\training\\\\dataset15.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df1 = df1.drop(df1.columns[0],axis=1)\n",
    "df2 = df2.drop(df2.columns[0],axis=1)\n",
    "df3 = df3.drop(df3.columns[0],axis=1)\n",
    "df4 = df4.drop(df4.columns[0],axis=1)\n",
    "df5 = df5.drop(df5.columns[0],axis=1)\n",
    "df6 = df6.drop(df6.columns[0],axis=1)\n",
    "df7 = df7.drop(df7.columns[0],axis=1)\n",
    "df8 = df8.drop(df8.columns[0],axis=1)\n",
    "df9 = df9.drop(df9.columns[0],axis=1)\n",
    "df10 = df10.drop(df10.columns[0],axis=1)\n",
    "df11 = df11.drop(df11.columns[0],axis=1)\n",
    "df12 = df12.drop(df12.columns[0],axis=1)\n",
    "df13 = df13.drop(df13.columns[0],axis=1)\n",
    "df14 = df14.drop(df14.columns[0],axis=1)\n",
    "df15 = df15.drop(df15.columns[0],axis=1)\n",
    "df16 = df16.drop(df16.columns[0],axis=1)\n",
    "df17 = df17.drop(df17.columns[0],axis=1)\n",
    "df18 = df18.drop(df18.columns[0],axis=1)\n",
    "df19 = df19.drop(df19.columns[0],axis=1)\n",
    "df20 = df20.drop(df20.columns[0],axis=1)\n",
    "df21 = df21.drop(df21.columns[0],axis=1)\n",
    "df22 = df22.drop(df22.columns[0],axis=1)\n",
    "df23 = df23.drop(df23.columns[0],axis=1)\n",
    "df24 = df24.drop(df24.columns[0],axis=1)\n",
    "df25 = df25.drop(df25.columns[0],axis=1)\n",
    "df26 = df26.drop(df26.columns[0],axis=1)\n",
    "df27 = df27.drop(df27.columns[0],axis=1)\n",
    "df28 = df28.drop(df28.columns[0],axis=1)\n",
    "df29 = df29.drop(df29.columns[0],axis=1)\n",
    "df30 = df30.drop(df30.columns[0],axis=1)\n",
    "df31 = df31.drop(df31.columns[0],axis=1)\n",
    "df32 = df32.drop(df32.columns[0],axis=1)\n",
    "df33 = df33.drop(df33.columns[0],axis=1)\n",
    "df34 = df34.drop(df34.columns[0],axis=1)\n",
    "df35 = df35.drop(df35.columns[0],axis=1)\n",
    "df36 = df36.drop(df36.columns[0],axis=1)\n",
    "df37 = df37.drop(df37.columns[0],axis=1)\n",
    "df38 = df38.drop(df38.columns[0],axis=1)\n",
    "df39 = df39.drop(df39.columns[0],axis=1)\n",
    "df40 = df40.drop(df40.columns[0],axis=1)\n",
    "df41 = df41.drop(df41.columns[0],axis=1)\n",
    "df42 = df42.drop(df42.columns[0],axis=1)\n",
    "df43 = df43.drop(df43.columns[0],axis=1)\n",
    "df44 = df44.drop(df44.columns[0],axis=1)\n",
    "df45 = df45.drop(df45.columns[0],axis=1)\n",
    "df46 = df46.drop(df46.columns[0],axis=1)\n",
    "df47 = df47.drop(df47.columns[0],axis=1)\n",
    "df48 = df48.drop(df48.columns[0],axis=1)\n",
    "df49 = df49.drop(df49.columns[0],axis=1)\n",
    "df50 = df50.drop(df50.columns[0],axis=1)\n",
    "df51 = df51.drop(df51.columns[0],axis=1)\n",
    "df52 = df52.drop(df52.columns[0],axis=1)\n",
    "df53 = df53.drop(df53.columns[0],axis=1)\n",
    "df54 = df54.drop(df54.columns[0],axis=1)\n",
    "df55 = df55.drop(df55.columns[0],axis=1)\n",
    "df56 = df56.drop(df56.columns[0],axis=1)\n",
    "df57 = df57.drop(df57.columns[0],axis=1)\n",
    "df58 = df58.drop(df58.columns[0],axis=1)\n",
    "df59 = df59.drop(df59.columns[0],axis=1)\n",
    "df60 = df60.drop(df60.columns[0],axis=1)\n",
    "df61 = df61.drop(df61.columns[0],axis=1)\n",
    "df62 = df62.drop(df62.columns[0],axis=1)\n",
    "df63 = df63.drop(df63.columns[0],axis=1)\n",
    "df64 = df64.drop(df64.columns[0],axis=1)\n",
    "df65 = df65.drop(df65.columns[0],axis=1)\n",
    "df66 = df66.drop(df66.columns[0],axis=1)\n",
    "df67 = df67.drop(df67.columns[0],axis=1)\n",
    "df68 = df68.drop(df68.columns[0],axis=1)\n",
    "df69 = df69.drop(df69.columns[0],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_list = [df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17,df18,df19,df20,df21,df22,df23,df24,df25,df26,df27,df28,df29,df30,df31,df32,df33,df34,df35,df36,df37,df38,df39,df40,df41,df42,df43,df44,df45,df46,df47,df48,df49,df50,df51,df52,df53,df54,df55,df56,df57,df58,df59,df60,df61,df62,df63,df64,df65,df66,df67,df68,df69]\n",
    "\n",
    "out = np.zeros(69)\n",
    "\n",
    "for z in np.arange(0,9):\n",
    "    out[z] = 1\n",
    "\n",
    "\n",
    "\n",
    "y = pd.DataFrame(out)\n",
    "\n",
    "\n",
    "\n",
    "l = np.arange(1,21)\n",
    "datalist = np.arange(0,69)\n",
    "l_new = [1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15,16,16,17,17,18,18,19,19,20,20]\n",
    "l_features_coef = pd.DataFrame(index=l_new,columns=np.arange(0, 18*20+1))\n",
    "errors_test_estimated = pd.DataFrame(index=l, columns = ['Estimated Test Error'])\n",
    "\n",
    "\n",
    "for i in l:\n",
    "    \n",
    "    splits = np.arange(0,i)\n",
    "    segment = math.floor(480/i)\n",
    "    df_list_new = []\n",
    "    j = 0\n",
    "    for j in datalist:\n",
    "        list_df_specific = []\n",
    "        df_parts = 0\n",
    "        for df_parts in splits:\n",
    "             list_df_specific.append(df_list[j].iloc[((df_parts)*(segment)) : ((df_parts + 1)*(segment)),:].reset_index(drop=True))\n",
    "        df_list_new.append(pd.concat(list_df_specific, axis=1))\n",
    "    \n",
    "    header_column = np.arange(0, 18*i) \n",
    "    features = pd.DataFrame(index=datalist,columns=header_column) \n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "    t=0\n",
    "    for t in datalist:\n",
    "        features.iloc[t, 0 : (6*i)]       =    (((pd.DataFrame(df_list_new[t].mean()).transpose()).to_numpy()).flatten())[:]\n",
    "        features.iloc[t, (6*i) : (12*i)]  =    (((pd.DataFrame(df_list_new[t].std()).transpose()).to_numpy()).flatten())[:]\n",
    "        features.iloc[t, (12*i) : (18*i)] =    (((pd.DataFrame(df_list_new[t].max()).transpose()).to_numpy()).flatten())[:]\n",
    "    \n",
    "    \n",
    "    total = features.copy()\n",
    "    total['Output'] = out\n",
    "    \n",
    "    # total has split features with output class, this is only the training data\n",
    "    \n",
    "    # total has the relevant split data that has to be cross validated upon\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True)  # performing 5-fold CV \n",
    "    \n",
    "    X_train = total.drop(columns=['Output'])\n",
    "    y_train = total['Output']                   \n",
    "    \n",
    "    # an l is fixed, we have external loop cross validating to estimate test error, and an internal loop\n",
    "    # of cross validation which cross validates for each cv iteration, external, the value of lambda\n",
    "    # and additionally each cv iteration will produce an estimate for test error which will be averages\n",
    "    # the coefficients and their indices will be placed in the l_features_coef matrix\n",
    "    # the errors will be saved in the error_test_estimate vector\n",
    "    \n",
    "    \n",
    "    # here external cv is stratified, as it wasn't used before we'll use it here\n",
    "    # additionally internal cv is automatically stratified by sklearn\n",
    "    l_spec_params = pd.DataFrame(index = np.arange(0,5),columns=np.arange(0,18*i+1))\n",
    "    cv_error = np.zeros(5) #to store cv errors\n",
    "    \n",
    "    # this will hold the param coeff for each cv iteration in that corresponding row index\n",
    "    # 0,1,2,3,4 holds for cv iteration 1,2,3,4,5 also columns have been indexed\n",
    "    # 0,1,2,3.........18*i as for some i we have 6*3*i = 18*i features\n",
    "    # but we also have parameter corresponding to intercept\n",
    "    # intercept will be stored at index 0, after that all features from 1 to 18*i corresponding\n",
    "    # to dataset features from index 0 to 18*i - 1, that again correspond to features\n",
    "    # in a way that we have defined and obtained in our split\n",
    "    \n",
    "    count_ext = 0\n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        X_train_cv,X_test_cv = X_train.iloc[train_index,:],X_train.iloc[test_index,:]\n",
    "        y_train_cv,y_test_cv = y_train.iloc[train_index],y_train.iloc[test_index]\n",
    "        count_ext = count_ext + 1\n",
    "        log_reg = LogisticRegressionCV(cv=5,solver='liblinear',penalty='l1').fit(X_train_cv, y_train_cv)\n",
    "        \n",
    "        # 5-fold CV picks best lambda/alpha/C \n",
    "        \n",
    "        # note liblinear and sklearn penalizes beta0 also, by default, so it will be considered a 'parameter'\n",
    "        \n",
    "        # selected parameter coefficients will be selected corresponding to that external CV iteration\n",
    "        # which gives us the least value of test error estimate, while test error estimate is averaged over all cv iterations\n",
    "        \n",
    "        # column headers have been integrally indexed, meaning of how they correspond to time series in split case\n",
    "        # have been discussed a priori\n",
    "        l_spec_params.iloc[count_ext-1,0] = log_reg.intercept_[0]\n",
    "        \n",
    "        coef_arr = log_reg.coef_.flatten()\n",
    "        \n",
    "        # print(coef_arr)\n",
    "        \n",
    "        l_spec_params.iloc[count_ext-1,1:(np.size(coef_arr)+1)] = coef_arr[0:np.size(coef_arr)]\n",
    "        \n",
    "        y_test_cv_predicted = log_reg.predict(X_test_cv)\n",
    "        \n",
    "        mis = 0\n",
    "        h=0\n",
    "        for h in np.arange(0,y_test_cv.shape[0]):\n",
    "            if y_test_cv_predicted[h] != y_test_cv.iloc[h]:\n",
    "                mis = mis + 1\n",
    "        \n",
    "        error = (mis/(y_test_cv.shape[0]))*100\n",
    "        \n",
    "        cv_error[count_ext-1]=error\n",
    "        \n",
    "    \n",
    "    \n",
    "    selected_index_params = np.argmin(cv_error)\n",
    "   \n",
    "    param_selected = (l_spec_params.iloc[selected_index_params,:]).to_numpy().flatten()\n",
    "    error_l = np.mean(cv_error)\n",
    "    errors_test_estimated.iloc[i-1,0]=error_l\n",
    "    total_feature_indices = np.arange(0,18*i+1)\n",
    "    \n",
    "    # total_feature_indices is an array indexed from 0,1,2,3,........,18*i\n",
    "    # 0 --- beta0 or intercept\n",
    "    # 1 --- feature 1, column header/index (same) 0  (in features/X_train/X_test/total)\n",
    "    # 2 --- feature 2, .......................... 1  ..................................\n",
    "    # .\n",
    "    # .\n",
    "    # .\n",
    "    # 18*i --- feature 18*i, .................... 18*i - 1 ............................\n",
    "    \n",
    "    # how each feature in features/X_train/X_test/total corresponds to original datasets has already been defined\n",
    "    # (mean of all splits, then std of all splits, then max of all splits)\n",
    "    \n",
    "    l_features_coef.iloc[2*i-2,0:(np.size(total_feature_indices))] = total_feature_indices[0:(np.size(total_feature_indices))]\n",
    "    l_features_coef.iloc[2*i-1,0:(np.size(param_selected))] = param_selected[0:(np.size(param_selected))]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "print('The matrix of features and their coefficients for different l is : \\n',l_features_coef)\n",
    "print('The test error estimates : \\n',errors_test_estimated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen the error estimates for different splits are obtained. This will change minorly with each runtime\n",
    "\n",
    "Additionally the parameter and its coefficient matrix is also obtained. It is sparse and symbolizes that quite a few parameters\n",
    "have been eliminated from the model by L1-penalization\n",
    "\n",
    "We'll save this to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_test_estimated.to_csv('3_e_i_l_error_test_estimated.csv')\n",
    "l_features_coef.to_csv('3_e_i_l_parameters_coefficients.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
