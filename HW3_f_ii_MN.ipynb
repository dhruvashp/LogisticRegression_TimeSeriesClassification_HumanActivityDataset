{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test error estimates : \n",
      "    Estimated Test Error\n",
      "1               20.1099\n",
      "2               17.2527\n",
      "3               20.2198\n",
      "4               13.0769\n",
      "5               14.5055\n",
      "6               14.6154\n",
      "7               18.9011\n",
      "8               18.7912\n",
      "9               15.7143\n",
      "10               19.011\n",
      "11              18.7912\n",
      "12              20.1099\n",
      "13              21.7582\n",
      "14              20.3297\n",
      "15              21.5385\n",
      "16              20.1099\n",
      "17              18.5714\n",
      "18              17.3626\n",
      "19              12.8571\n",
      "20              20.3297\n",
      "The l split selected that minimizes the test error estimate for this multiclass classification is : \n",
      " 19\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('bending1\\\\training\\\\dataset3.csv')\n",
    "df2 = pd.read_csv('bending1\\\\training\\\\dataset4.csv')\n",
    "df3 = pd.read_csv('bending1\\\\training\\\\dataset5.csv')\n",
    "df4 = pd.read_csv('bending1\\\\training\\\\dataset6.csv')\n",
    "df5 = pd.read_csv('bending1\\\\training\\\\dataset7.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df6 = pd.read_csv('bending2\\\\training\\\\dataset3.csv')\n",
    "df7 = pd.read_csv('bending2\\\\training\\\\dataset4.csv')\n",
    "df8 = pd.read_csv('bending2\\\\training\\\\dataset5.csv')\n",
    "df9 = pd.read_csv('bending2\\\\training\\\\dataset6.csv')\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "df10 = pd.read_csv('cycling\\\\training\\\\dataset4.csv')\n",
    "df11 = pd.read_csv('cycling\\\\training\\\\dataset5.csv')\n",
    "df12 = pd.read_csv('cycling\\\\training\\\\dataset6.csv')\n",
    "df13 = pd.read_csv('cycling\\\\training\\\\dataset7.csv')\n",
    "df14 = pd.read_csv('cycling\\\\training\\\\dataset8.csv')\n",
    "df15 = pd.read_csv('cycling\\\\training\\\\dataset9.csv')\n",
    "df16 = pd.read_csv('cycling\\\\training\\\\dataset10.csv')\n",
    "df17 = pd.read_csv('cycling\\\\training\\\\dataset11.csv')\n",
    "df18 = pd.read_csv('cycling\\\\training\\\\dataset12.csv')\n",
    "df19 = pd.read_csv('cycling\\\\training\\\\dataset13.csv')\n",
    "df20 = pd.read_csv('cycling\\\\training\\\\dataset14.csv')\n",
    "df21 = pd.read_csv('cycling\\\\training\\\\dataset15.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df22 = pd.read_csv('lying\\\\training\\\\dataset4.csv')\n",
    "df23 = pd.read_csv('lying\\\\training\\\\dataset5.csv')\n",
    "df24 = pd.read_csv('lying\\\\training\\\\dataset6.csv')\n",
    "df25 = pd.read_csv('lying\\\\training\\\\dataset7.csv')\n",
    "df26 = pd.read_csv('lying\\\\training\\\\dataset8.csv')\n",
    "df27 = pd.read_csv('lying\\\\training\\\\dataset9.csv')\n",
    "df28 = pd.read_csv('lying\\\\training\\\\dataset10.csv')\n",
    "df29 = pd.read_csv('lying\\\\training\\\\dataset11.csv')\n",
    "df30 = pd.read_csv('lying\\\\training\\\\dataset12.csv')\n",
    "df31 = pd.read_csv('lying\\\\training\\\\dataset13.csv')\n",
    "df32 = pd.read_csv('lying\\\\training\\\\dataset14.csv')\n",
    "df33 = pd.read_csv('lying\\\\training\\\\dataset15.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "df34 = pd.read_csv('sitting\\\\training\\\\dataset4.csv')\n",
    "df35 = pd.read_csv('sitting\\\\training\\\\dataset5.csv')\n",
    "df36 = pd.read_csv('sitting\\\\training\\\\dataset6.csv')\n",
    "df37 = pd.read_csv('sitting\\\\training\\\\dataset7.csv')\n",
    "df38 = pd.read_csv('sitting\\\\training\\\\dataset8.csv')\n",
    "df39 = pd.read_csv('sitting\\\\training\\\\dataset9.csv')\n",
    "df40 = pd.read_csv('sitting\\\\training\\\\dataset10.csv')\n",
    "df41 = pd.read_csv('sitting\\\\training\\\\dataset11.csv')\n",
    "df42 = pd.read_csv('sitting\\\\training\\\\dataset12.csv')\n",
    "df43 = pd.read_csv('sitting\\\\training\\\\dataset13.csv')\n",
    "df44 = pd.read_csv('sitting\\\\training\\\\dataset14.csv')\n",
    "df45 = pd.read_csv('sitting\\\\training\\\\dataset15.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df46 = pd.read_csv('standing\\\\training\\\\dataset4.csv')\n",
    "df47 = pd.read_csv('standing\\\\training\\\\dataset5.csv')\n",
    "df48 = pd.read_csv('standing\\\\training\\\\dataset6.csv')\n",
    "df49 = pd.read_csv('standing\\\\training\\\\dataset7.csv')\n",
    "df50 = pd.read_csv('standing\\\\training\\\\dataset8.csv')\n",
    "df51 = pd.read_csv('standing\\\\training\\\\dataset9.csv')\n",
    "df52 = pd.read_csv('standing\\\\training\\\\dataset10.csv')\n",
    "df53 = pd.read_csv('standing\\\\training\\\\dataset11.csv')\n",
    "df54 = pd.read_csv('standing\\\\training\\\\dataset12.csv')\n",
    "df55 = pd.read_csv('standing\\\\training\\\\dataset13.csv')\n",
    "df56 = pd.read_csv('standing\\\\training\\\\dataset14.csv')\n",
    "df57 = pd.read_csv('standing\\\\training\\\\dataset15.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df58 = pd.read_csv('walking\\\\training\\\\dataset4.csv')\n",
    "df59 = pd.read_csv('walking\\\\training\\\\dataset5.csv')\n",
    "df60 = pd.read_csv('walking\\\\training\\\\dataset6.csv')\n",
    "df61 = pd.read_csv('walking\\\\training\\\\dataset7.csv')\n",
    "df62 = pd.read_csv('walking\\\\training\\\\dataset8.csv')\n",
    "df63 = pd.read_csv('walking\\\\training\\\\dataset9.csv')\n",
    "df64 = pd.read_csv('walking\\\\training\\\\dataset10.csv')\n",
    "df65 = pd.read_csv('walking\\\\training\\\\dataset11.csv')\n",
    "df66 = pd.read_csv('walking\\\\training\\\\dataset12.csv')\n",
    "df67 = pd.read_csv('walking\\\\training\\\\dataset13.csv')\n",
    "df68 = pd.read_csv('walking\\\\training\\\\dataset14.csv')\n",
    "df69 = pd.read_csv('walking\\\\training\\\\dataset15.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df1 = df1.drop(df1.columns[0],axis=1)\n",
    "df2 = df2.drop(df2.columns[0],axis=1)\n",
    "df3 = df3.drop(df3.columns[0],axis=1)\n",
    "df4 = df4.drop(df4.columns[0],axis=1)\n",
    "df5 = df5.drop(df5.columns[0],axis=1)\n",
    "df6 = df6.drop(df6.columns[0],axis=1)\n",
    "df7 = df7.drop(df7.columns[0],axis=1)\n",
    "df8 = df8.drop(df8.columns[0],axis=1)\n",
    "df9 = df9.drop(df9.columns[0],axis=1)\n",
    "df10 = df10.drop(df10.columns[0],axis=1)\n",
    "df11 = df11.drop(df11.columns[0],axis=1)\n",
    "df12 = df12.drop(df12.columns[0],axis=1)\n",
    "df13 = df13.drop(df13.columns[0],axis=1)\n",
    "df14 = df14.drop(df14.columns[0],axis=1)\n",
    "df15 = df15.drop(df15.columns[0],axis=1)\n",
    "df16 = df16.drop(df16.columns[0],axis=1)\n",
    "df17 = df17.drop(df17.columns[0],axis=1)\n",
    "df18 = df18.drop(df18.columns[0],axis=1)\n",
    "df19 = df19.drop(df19.columns[0],axis=1)\n",
    "df20 = df20.drop(df20.columns[0],axis=1)\n",
    "df21 = df21.drop(df21.columns[0],axis=1)\n",
    "df22 = df22.drop(df22.columns[0],axis=1)\n",
    "df23 = df23.drop(df23.columns[0],axis=1)\n",
    "df24 = df24.drop(df24.columns[0],axis=1)\n",
    "df25 = df25.drop(df25.columns[0],axis=1)\n",
    "df26 = df26.drop(df26.columns[0],axis=1)\n",
    "df27 = df27.drop(df27.columns[0],axis=1)\n",
    "df28 = df28.drop(df28.columns[0],axis=1)\n",
    "df29 = df29.drop(df29.columns[0],axis=1)\n",
    "df30 = df30.drop(df30.columns[0],axis=1)\n",
    "df31 = df31.drop(df31.columns[0],axis=1)\n",
    "df32 = df32.drop(df32.columns[0],axis=1)\n",
    "df33 = df33.drop(df33.columns[0],axis=1)\n",
    "df34 = df34.drop(df34.columns[0],axis=1)\n",
    "df35 = df35.drop(df35.columns[0],axis=1)\n",
    "df36 = df36.drop(df36.columns[0],axis=1)\n",
    "df37 = df37.drop(df37.columns[0],axis=1)\n",
    "df38 = df38.drop(df38.columns[0],axis=1)\n",
    "df39 = df39.drop(df39.columns[0],axis=1)\n",
    "df40 = df40.drop(df40.columns[0],axis=1)\n",
    "df41 = df41.drop(df41.columns[0],axis=1)\n",
    "df42 = df42.drop(df42.columns[0],axis=1)\n",
    "df43 = df43.drop(df43.columns[0],axis=1)\n",
    "df44 = df44.drop(df44.columns[0],axis=1)\n",
    "df45 = df45.drop(df45.columns[0],axis=1)\n",
    "df46 = df46.drop(df46.columns[0],axis=1)\n",
    "df47 = df47.drop(df47.columns[0],axis=1)\n",
    "df48 = df48.drop(df48.columns[0],axis=1)\n",
    "df49 = df49.drop(df49.columns[0],axis=1)\n",
    "df50 = df50.drop(df50.columns[0],axis=1)\n",
    "df51 = df51.drop(df51.columns[0],axis=1)\n",
    "df52 = df52.drop(df52.columns[0],axis=1)\n",
    "df53 = df53.drop(df53.columns[0],axis=1)\n",
    "df54 = df54.drop(df54.columns[0],axis=1)\n",
    "df55 = df55.drop(df55.columns[0],axis=1)\n",
    "df56 = df56.drop(df56.columns[0],axis=1)\n",
    "df57 = df57.drop(df57.columns[0],axis=1)\n",
    "df58 = df58.drop(df58.columns[0],axis=1)\n",
    "df59 = df59.drop(df59.columns[0],axis=1)\n",
    "df60 = df60.drop(df60.columns[0],axis=1)\n",
    "df61 = df61.drop(df61.columns[0],axis=1)\n",
    "df62 = df62.drop(df62.columns[0],axis=1)\n",
    "df63 = df63.drop(df63.columns[0],axis=1)\n",
    "df64 = df64.drop(df64.columns[0],axis=1)\n",
    "df65 = df65.drop(df65.columns[0],axis=1)\n",
    "df66 = df66.drop(df66.columns[0],axis=1)\n",
    "df67 = df67.drop(df67.columns[0],axis=1)\n",
    "df68 = df68.drop(df68.columns[0],axis=1)\n",
    "df69 = df69.drop(df69.columns[0],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_list = [df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17,df18,df19,df20,df21,df22,df23,df24,df25,df26,df27,df28,df29,df30,df31,df32,df33,df34,df35,df36,df37,df38,df39,df40,df41,df42,df43,df44,df45,df46,df47,df48,df49,df50,df51,df52,df53,df54,df55,df56,df57,df58,df59,df60,df61,df62,df63,df64,df65,df66,df67,df68,df69]\n",
    "\n",
    "out =     [0,  0,  0,  0,  0,  0,  0,  0,  0,  1,   1,    1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   2,   2,   2,   2,   2,  2,     2,    2,   2,   2,   2,  2,  3,   3,    3,    3,   3,  3,   3,   3,   3,   3,   3,   3,   4,    4,   4,  4,    4,   4,   4,   4,  4,   4,   4,   4,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5]\n",
    "\n",
    "# 69 datasets, each dataset will me split, features extracted, and instantiated in that same index in a corresponding manner\n",
    "# thus out has 69 entries, corresponding to class in each dataset\n",
    "# 0 : bending (1 & 2)\n",
    "# 1 : cycling\n",
    "# 2 : lying\n",
    "# 3 : sitting\n",
    "# 4 : standing\n",
    "# 5 : walking\n",
    "# ordered as can be seen again their dataframes, corresponding to df_list which will be instantiated in similar index orders\n",
    "# thus this is a multiclass, 6 class problem, total 6 classes from 0 to 5\n",
    "\n",
    "y = pd.DataFrame(out)\n",
    "\n",
    "\n",
    "\n",
    "l = np.arange(1,21)\n",
    "datalist = np.arange(0,69)\n",
    "l_new = [1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15,16,16,17,17,18,18,19,19,20,20]\n",
    "errors_test_estimated = pd.DataFrame(index=l, columns = ['Estimated Test Error'])\n",
    "\n",
    "\n",
    "for i in l:\n",
    "    \n",
    "    splits = np.arange(0,i)\n",
    "    segment = math.floor(480/i)\n",
    "    df_list_new = []\n",
    "    j = 0\n",
    "    for j in datalist:\n",
    "        list_df_specific = []\n",
    "        df_parts = 0\n",
    "        for df_parts in splits:\n",
    "             list_df_specific.append(df_list[j].iloc[((df_parts)*(segment)) : ((df_parts + 1)*(segment)),:].reset_index(drop=True))\n",
    "        df_list_new.append(pd.concat(list_df_specific, axis=1))\n",
    "    \n",
    "    header_column = np.arange(0, 18*i) \n",
    "    features = pd.DataFrame(index=datalist,columns=header_column) \n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "    t=0\n",
    "    for t in datalist:\n",
    "        features.iloc[t, 0 : (6*i)]       =    (((pd.DataFrame(df_list_new[t].mean()).transpose()).to_numpy()).flatten())[:]\n",
    "        features.iloc[t, (6*i) : (12*i)]  =    (((pd.DataFrame(df_list_new[t].std()).transpose()).to_numpy()).flatten())[:]\n",
    "        features.iloc[t, (12*i) : (18*i)] =    (((pd.DataFrame(df_list_new[t].max()).transpose()).to_numpy()).flatten())[:]\n",
    "    \n",
    "    \n",
    "    total = features.copy()\n",
    "    total['Output'] = out\n",
    "    \n",
    "    # note this out will stay same, irrespective of split, as each spit in each dataset in markedly instantiated against\n",
    "    # its corresponding out\n",
    "    # thus no matter the split index row 0 will have instantiated dataset df1 of bending 1 which against it will contain its\n",
    "    # class 0, also in corresponding index row 0\n",
    "    # and so on and so forth for all the corresponding classes in general\n",
    "    \n",
    "    # total has split features with output class, this is only the training data\n",
    "    \n",
    "    # total has the relevant split data that has to be cross validated upon\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True)  # performing 5-fold CV \n",
    "    \n",
    "    X_train = total.drop(columns=['Output'])\n",
    "    y_train = total['Output']                   \n",
    "    \n",
    "    \n",
    "    # the errors will be saved in the error_test_estimate vector\n",
    "    # here external cv is stratified, as it wasn't used before we'll use it here\n",
    "    # additionally internal cv is automatically stratified by sklearn\n",
    "    \n",
    "    cv_error = np.zeros(5) #to store cv errors\n",
    "    \n",
    "    \n",
    "    count_ext = 0\n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        X_train_cv,X_test_cv = X_train.iloc[train_index,:],X_train.iloc[test_index,:]\n",
    "        y_train_cv,y_test_cv = y_train.iloc[train_index],y_train.iloc[test_index]\n",
    "        count_ext = count_ext + 1\n",
    "        cls =  MultinomialNB().fit(X_train_cv, y_train_cv)\n",
    "        \n",
    "        y_test_cv_predicted = cls.predict(X_test_cv)\n",
    "        \n",
    "        mis = 0\n",
    "        h=0\n",
    "        for h in np.arange(0,y_test_cv.shape[0]):\n",
    "            if y_test_cv_predicted[h] != y_test_cv.iloc[h]:\n",
    "                mis = mis + 1\n",
    "        \n",
    "        error = (mis/(y_test_cv.shape[0]))*100\n",
    "        \n",
    "        cv_error[count_ext-1]=error\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "   \n",
    "    error_l = np.mean(cv_error)\n",
    "    errors_test_estimated.iloc[i-1,0]=error_l\n",
    "    \n",
    "    \n",
    "    \n",
    "print('The test error estimates : \\n',errors_test_estimated)\n",
    "\n",
    "\n",
    "index = np.argmin(errors_test_estimated)\n",
    "l_selected = index + 1\n",
    "print('The l split selected that minimizes the test error estimate for this multiclass classification is : \\n',l_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, over multiple runs, l-split is obtained higher, much higher for Multinomial NB as opposed to for Gaussian NB and for Logistic Regression. Errors lower than LR but slightly higher than Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0       1        2       3        4       5        6       7  \\\n",
      "0   43.5732   0.372    22.13  0.6264    35.22  0.3952  44.5464  0.3592   \n",
      "1   44.2032  0.6424    24.88  0.6328    36.16  0.5476    44.31  0.5428   \n",
      "2   41.5012   0.486  21.9132  0.8676  31.9164  0.5548   42.056  0.4872   \n",
      "3   44.7464  0.2972  16.9996  0.8812    26.88  0.5824    44.98    0.02   \n",
      "4   42.8932   0.484  16.8868  0.7956  18.3776  0.8656  43.1216  0.4784   \n",
      "..      ...     ...      ...     ...      ...     ...      ...     ...   \n",
      "64  34.3432  4.0924  15.9468   3.022    17.75  3.0604    35.51  4.3864   \n",
      "65  34.9668  6.0148  15.2968  3.6828  15.8432  4.1696  33.7668   5.546   \n",
      "66  33.9168  3.9236  15.3732  2.6724     17.5  3.6444   34.244  4.3248   \n",
      "67  34.9732  4.4404  15.9748   3.058  16.3668  3.3736  33.9204   4.514   \n",
      "68  36.8268  4.0368    15.34   2.714  16.3936  3.2192  35.0236  4.2132   \n",
      "\n",
      "          8       9  ...   333    334   335    336    337    338   339    340  \\\n",
      "0   24.0068  0.1436  ...  1.25  36.33  0.47   43.5      1  20.25  0.71   36.5   \n",
      "1   24.3524    0.65  ...  5.15   31.5   1.5     45   2.29  26.25  4.06   35.5   \n",
      "2   18.3576  1.2028  ...  2.06   28.5  0.83   41.5   1.09  20.75  2.35  29.25   \n",
      "3   18.5512  1.1832  ...  1.41     25     1   43.5   1.58  23.25  2.29  23.67   \n",
      "4   14.5748  1.0476  ...  1.66     24  1.25  44.75   1.12  18.67  3.27  24.67   \n",
      "..      ...     ...  ...   ...    ...   ...    ...    ...    ...   ...    ...   \n",
      "64  15.3832  3.3972  ...  4.44  21.33  6.28     40   9.18  20.25  6.72  19.67   \n",
      "65  15.3996   2.906  ...  7.46  21.33  8.08  42.25   8.26  20.67  7.18   20.5   \n",
      "66   15.506  2.9696  ...  6.69   19.5  7.09     44  11.05     21  8.22     21   \n",
      "67  14.6268  3.1204  ...  6.14   23.5  4.74     44  11.02  21.25     7     19   \n",
      "68    15.02  3.3504  ...  4.82  19.33  6.44  42.33  10.05   19.5  7.65  21.25   \n",
      "\n",
      "     341 Output  \n",
      "0   0.87      0  \n",
      "1    1.5      0  \n",
      "2      1      0  \n",
      "3   4.06      0  \n",
      "4   1.73      0  \n",
      "..   ...    ...  \n",
      "64  5.87      5  \n",
      "65  8.38      5  \n",
      "66  7.72      5  \n",
      "67  6.87      5  \n",
      "68  8.04      5  \n",
      "\n",
      "[69 rows x 343 columns]\n"
     ]
    }
   ],
   "source": [
    "i=19    # train split into 19\n",
    "    \n",
    "splits = np.arange(0,i)\n",
    "segment = math.floor(480/i)\n",
    "df_list_new = []\n",
    "j = 0\n",
    "for j in datalist:\n",
    "    list_df_specific = []\n",
    "    df_parts = 0\n",
    "    for df_parts in splits:\n",
    "         list_df_specific.append(df_list[j].iloc[((df_parts)*(segment)) : ((df_parts + 1)*(segment)),:].reset_index(drop=True))\n",
    "    df_list_new.append(pd.concat(list_df_specific, axis=1))\n",
    "\n",
    "header_column = np.arange(0, 18*i) \n",
    "features = pd.DataFrame(index=datalist,columns=header_column) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "t=0\n",
    "for t in datalist:\n",
    "    features.iloc[t, 0 : (6*i)]       =    (((pd.DataFrame(df_list_new[t].mean()).transpose()).to_numpy()).flatten())[:]\n",
    "    features.iloc[t, (6*i) : (12*i)]  =    (((pd.DataFrame(df_list_new[t].std()).transpose()).to_numpy()).flatten())[:]\n",
    "    features.iloc[t, (12*i) : (18*i)] =    (((pd.DataFrame(df_list_new[t].max()).transpose()).to_numpy()).flatten())[:]\n",
    "    \n",
    "total = features.copy()\n",
    "total['Output'] = out\n",
    "\n",
    "print(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0       1        2       3        4       5        6       7    \\\n",
      "0   43.5732   0.372    22.13  0.6264    35.22  0.3952  44.5464  0.3592   \n",
      "1   44.2032  0.6424    24.88  0.6328    36.16  0.5476    44.31  0.5428   \n",
      "2   41.5012   0.486  21.9132  0.8676  31.9164  0.5548   42.056  0.4872   \n",
      "3   44.7464  0.2972  16.9996  0.8812    26.88  0.5824    44.98    0.02   \n",
      "4   42.8932   0.484  16.8868  0.7956  18.3776  0.8656  43.1216  0.4784   \n",
      "..      ...     ...      ...     ...      ...     ...      ...     ...   \n",
      "64  34.3432  4.0924  15.9468   3.022    17.75  3.0604    35.51  4.3864   \n",
      "65  34.9668  6.0148  15.2968  3.6828  15.8432  4.1696  33.7668   5.546   \n",
      "66  33.9168  3.9236  15.3732  2.6724     17.5  3.6444   34.244  4.3248   \n",
      "67  34.9732  4.4404  15.9748   3.058  16.3668  3.3736  33.9204   4.514   \n",
      "68  36.8268  4.0368    15.34   2.714  16.3936  3.2192  35.0236  4.2132   \n",
      "\n",
      "        8       9    ...    332   333    334   335    336    337    338   339  \\\n",
      "0   24.0068  0.1436  ...  19.33  1.25  36.33  0.47   43.5      1  20.25  0.71   \n",
      "1   24.3524    0.65  ...  24.33  5.15   31.5   1.5     45   2.29  26.25  4.06   \n",
      "2   18.3576  1.2028  ...  17.75  2.06   28.5  0.83   41.5   1.09  20.75  2.35   \n",
      "3   18.5512  1.1832  ...     14  1.41     25     1   43.5   1.58  23.25  2.29   \n",
      "4   14.5748  1.0476  ...   16.5  1.66     24  1.25  44.75   1.12  18.67  3.27   \n",
      "..      ...     ...  ...    ...   ...    ...   ...    ...    ...    ...   ...   \n",
      "64  15.3832  3.3972  ...     20  4.44  21.33  6.28     40   9.18  20.25  6.72   \n",
      "65  15.3996   2.906  ...   19.5  7.46  21.33  8.08  42.25   8.26  20.67  7.18   \n",
      "66   15.506  2.9696  ...   19.5  6.69   19.5  7.09     44  11.05     21  8.22   \n",
      "67  14.6268  3.1204  ...  20.25  6.14   23.5  4.74     44  11.02  21.25     7   \n",
      "68    15.02  3.3504  ...     20  4.82  19.33  6.44  42.33  10.05   19.5  7.65   \n",
      "\n",
      "      340   341  \n",
      "0    36.5  0.87  \n",
      "1    35.5   1.5  \n",
      "2   29.25     1  \n",
      "3   23.67  4.06  \n",
      "4   24.67  1.73  \n",
      "..    ...   ...  \n",
      "64  19.67  5.87  \n",
      "65   20.5  8.38  \n",
      "66     21  7.72  \n",
      "67     19  6.87  \n",
      "68  21.25  8.04  \n",
      "\n",
      "[69 rows x 342 columns]\n",
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "     ..\n",
      "64    5\n",
      "65    5\n",
      "66    5\n",
      "67    5\n",
      "68    5\n",
      "Name: Output, Length: 69, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train = total.drop(columns = ['Output'])\n",
    "y_train = total['Output']\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0       1        2       3        4       5        6       7  \\\n",
      "0   39.7068  0.4756  21.5168  0.3484       33  0.0892  41.1936  0.6204   \n",
      "1   43.4404  0.5236  17.4768  1.9484  31.8768  0.3416  43.4164  0.5752   \n",
      "2   19.4964  1.4108   23.362  0.4372  25.3296  0.8092  22.0468   0.672   \n",
      "3        27  0.0544  20.6644  0.7332  16.6296  1.2492  28.7328  0.1628   \n",
      "4   36.0932  2.5424  16.4332  4.0428  19.9932  3.2904  37.2168  2.1788   \n",
      "5   37.3064  2.1412  16.1996   2.732    18.93   2.532  37.3068  2.1996   \n",
      "6   38.0836  2.2836  16.2832  3.0152    20.07   2.392    37.03  2.1228   \n",
      "7     28.95  0.0716   7.7492  0.6544   8.7676  0.3268  29.0532   0.162   \n",
      "8   29.0764  0.1092   4.7564  0.3556  10.8852  0.4852  28.8096  0.2224   \n",
      "9        48       0     5.76  0.1608   1.7836  0.6988       48       0   \n",
      "10    41.98  0.0344  17.3244  0.7272   9.0096   1.492    42.01  0.0172   \n",
      "11    47.51  0.6532   22.672  0.4244    13.63  1.0572       48       0   \n",
      "12    45.47   0.278  13.4164  0.9968    14.67   1.008  45.5132  0.4704   \n",
      "13  46.4636  0.4776    12.06  0.2296  18.7744  0.6916  46.6728   0.444   \n",
      "14   42.536  0.5644  13.6932    0.48  15.5036  0.7884  43.0632  0.4956   \n",
      "15  44.6936  0.3456   6.9064  1.2156  14.5968  0.8424  42.4668  0.6228   \n",
      "16  34.8112  4.0764  16.6168  2.8912    16.04   3.856  34.7768  3.2976   \n",
      "17   34.384  4.5216  15.5364  3.1164  16.1708  3.8424  33.6532  4.3672   \n",
      "18  32.9328  5.0708  16.7768  2.9352  16.7736  3.4108    32.94  4.7668   \n",
      "\n",
      "          8       9  ...   333    334   335    336    337    338   339    340  \\\n",
      "0     18.05  0.6504  ...  3.27  37.75   1.5     44      1     26  1.12     32   \n",
      "1   21.5332  0.7564  ...  5.72  33.75   1.3   45.5   1.12  25.75  4.55   34.5   \n",
      "2   20.6232    0.66  ...  0.71  23.75  1.25     27   0.49     24  0.87   22.5   \n",
      "3    16.984  1.4324  ...  4.64     33  6.68     24   1.22     21  2.06     29   \n",
      "4   17.8416  2.7312  ...  4.71     23  8.17  42.67   7.25  22.33  7.26   23.5   \n",
      "5   15.3764  3.7296  ...   5.8  23.25   5.2  42.33   3.57   23.5  7.69   23.5   \n",
      "6   17.2668  2.7236  ...  4.97  20.33   8.5  42.75   7.04     24  7.12  20.67   \n",
      "7    5.9856  1.1516  ...  5.02   16.5   3.3     30   0.94  10.75  2.35  11.75   \n",
      "8    6.2136  0.5804  ...  0.71      4  1.25     48      0   6.75  0.83   2.33   \n",
      "9    6.3436  0.3844  ...  0.94   11.5  1.12     48      0   9.75  1.58   11.5   \n",
      "10  15.7136  0.4852  ...  3.56     20  4.78  44.75   0.94   22.5   3.3     28   \n",
      "11  15.1164   0.738  ...  2.55  20.25  5.89     46    0.5     21  4.58  20.75   \n",
      "12  13.6992   0.662  ...   3.9   21.5   1.3     45      0   11.5  3.19     22   \n",
      "13  14.1008   0.822  ...  4.76  18.25  3.27  43.67      1  16.75  0.87   16.5   \n",
      "14    16.42  0.4996  ...   1.3  18.33   1.3     45   0.83     12     3     18   \n",
      "15  14.4932  0.7052  ...  3.49  18.67  3.74     42   1.25     21  1.22   19.5   \n",
      "16    14.33  3.5828  ...  8.26  23.25  8.86  42.33   9.68  21.75   6.5     24   \n",
      "17  16.9636  3.0916  ...   7.4     20  6.48  43.25   7.26  18.75  6.53  22.25   \n",
      "18  14.3132  3.1272  ...  6.48  22.33  7.41   41.5  11.03     20  7.22  21.33   \n",
      "\n",
      "     341 Output  \n",
      "0   0.94      0  \n",
      "1    1.5      0  \n",
      "2   1.12      0  \n",
      "3   4.74      0  \n",
      "4   7.79      1  \n",
      "5   8.83      1  \n",
      "6   8.26      1  \n",
      "7   1.92      2  \n",
      "8      1      2  \n",
      "9   1.22      2  \n",
      "10  5.79      3  \n",
      "11  7.22      3  \n",
      "12  1.09      3  \n",
      "13  1.12      4  \n",
      "14  1.64      4  \n",
      "15  2.06      4  \n",
      "16  6.69      5  \n",
      "17  7.95      5  \n",
      "18  8.04      5  \n",
      "\n",
      "[19 rows x 343 columns]\n"
     ]
    }
   ],
   "source": [
    "df_n_1 = pd.read_csv('bending1\\\\test\\\\dataset1.csv')\n",
    "df_n_2 = pd.read_csv('bending1\\\\test\\\\dataset2.csv')\n",
    "\n",
    "df_n_3 = pd.read_csv('bending2\\\\test\\\\dataset1.csv')\n",
    "df_n_4= pd.read_csv('bending2\\\\test\\\\dataset2.csv')\n",
    "\n",
    "df_n_5 = pd.read_csv('cycling\\\\test\\\\dataset1.csv')\n",
    "df_n_6 = pd.read_csv('cycling\\\\test\\\\dataset2.csv')\n",
    "df_n_7 = pd.read_csv('cycling\\\\test\\\\dataset3.csv')\n",
    "\n",
    "df_n_8 = pd.read_csv('lying\\\\test\\\\dataset1.csv')\n",
    "df_n_9 = pd.read_csv('lying\\\\test\\\\dataset2.csv')\n",
    "df_n_10 = pd.read_csv('lying\\\\test\\\\dataset3.csv')\n",
    "\n",
    "df_n_11 = pd.read_csv('sitting\\\\test\\\\dataset1.csv')\n",
    "df_n_12 = pd.read_csv('sitting\\\\test\\\\dataset2.csv')\n",
    "df_n_13 = pd.read_csv('sitting\\\\test\\\\dataset3.csv')\n",
    "\n",
    "df_n_14 = pd.read_csv('standing\\\\test\\\\dataset1.csv')\n",
    "df_n_15 = pd.read_csv('standing\\\\test\\\\dataset2.csv')\n",
    "df_n_16 = pd.read_csv('standing\\\\test\\\\dataset3.csv')\n",
    "\n",
    "df_n_17 = pd.read_csv('walking\\\\test\\\\dataset1.csv')\n",
    "df_n_18 = pd.read_csv('walking\\\\test\\\\dataset2.csv')\n",
    "df_n_19 = pd.read_csv('walking\\\\test\\\\dataset3.csv')\n",
    "\n",
    "df_n_1 = df_n_1.drop(df_n_1.columns[0],axis=1)\n",
    "df_n_2 = df_n_2.drop(df_n_2.columns[0],axis=1)\n",
    "df_n_3 = df_n_3.drop(df_n_3.columns[0],axis=1)\n",
    "df_n_4 = df_n_4.drop(df_n_4.columns[0],axis=1)\n",
    "df_n_5 = df_n_5.drop(df_n_5.columns[0],axis=1)\n",
    "df_n_6 = df_n_6.drop(df_n_6.columns[0],axis=1)\n",
    "df_n_7 = df_n_7.drop(df_n_7.columns[0],axis=1)\n",
    "df_n_8 = df_n_8.drop(df_n_8.columns[0],axis=1)\n",
    "df_n_9 = df_n_9.drop(df_n_9.columns[0],axis=1)\n",
    "df_n_10 = df_n_10.drop(df_n_10.columns[0],axis=1)\n",
    "df_n_11 = df_n_11.drop(df_n_11.columns[0],axis=1)\n",
    "df_n_12 = df_n_12.drop(df_n_12.columns[0],axis=1)\n",
    "df_n_13 = df_n_13.drop(df_n_13.columns[0],axis=1)\n",
    "df_n_14 = df_n_14.drop(df_n_14.columns[0],axis=1)\n",
    "df_n_15 = df_n_15.drop(df_n_15.columns[0],axis=1)\n",
    "df_n_16 = df_n_16.drop(df_n_16.columns[0],axis=1)\n",
    "df_n_17 = df_n_17.drop(df_n_17.columns[0],axis=1)\n",
    "df_n_18 = df_n_18.drop(df_n_18.columns[0],axis=1)\n",
    "df_n_19 = df_n_19.drop(df_n_19.columns[0],axis=1)\n",
    "\n",
    "df_list_test = [df_n_1,df_n_2,df_n_3,df_n_4,df_n_5,df_n_6,df_n_7,df_n_8,df_n_9,df_n_10,df_n_11,df_n_12,df_n_13,df_n_14,df_n_15,df_n_16,df_n_17,df_n_18,df_n_19]\n",
    "\n",
    "out_test =     [0,      0,      0,     0,     1,     1,     1,     2,     2,     2,     3,        3,      3,     4,      4,     4,       5,      5,       5]\n",
    "\n",
    "y_test = pd.DataFrame(out_test)\n",
    "\n",
    "datalist_test = np.arange(0,19)\n",
    "\n",
    "\n",
    "\n",
    "i=19      # test split into 19\n",
    "    \n",
    "splits = np.arange(0,i)\n",
    "segment = math.floor(480/i)\n",
    "df_list_new = []\n",
    "j = 0\n",
    "for j in datalist_test:\n",
    "    list_df_specific = []\n",
    "    df_parts = 0\n",
    "    for df_parts in splits:\n",
    "         list_df_specific.append(df_list_test[j].iloc[((df_parts)*(segment)) : ((df_parts + 1)*(segment)),:].reset_index(drop=True))\n",
    "    df_list_new.append(pd.concat(list_df_specific, axis=1))\n",
    "\n",
    "header_column = np.arange(0, 18*i) \n",
    "features_test = pd.DataFrame(index=datalist_test,columns=header_column) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "t=0\n",
    "for t in datalist_test:\n",
    "    features_test.iloc[t, 0 : (6*i)]       =    (((pd.DataFrame(df_list_new[t].mean()).transpose()).to_numpy()).flatten())[:]\n",
    "    features_test.iloc[t, (6*i) : (12*i)]  =    (((pd.DataFrame(df_list_new[t].std()).transpose()).to_numpy()).flatten())[:]\n",
    "    features_test.iloc[t, (12*i) : (18*i)] =    (((pd.DataFrame(df_list_new[t].max()).transpose()).to_numpy()).flatten())[:]\n",
    "    \n",
    "total_test = features_test.copy()\n",
    "total_test['Output'] = out_test\n",
    "\n",
    "print(total_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0       1        2       3        4       5        6       7    \\\n",
      "0   39.7068  0.4756  21.5168  0.3484       33  0.0892  41.1936  0.6204   \n",
      "1   43.4404  0.5236  17.4768  1.9484  31.8768  0.3416  43.4164  0.5752   \n",
      "2   19.4964  1.4108   23.362  0.4372  25.3296  0.8092  22.0468   0.672   \n",
      "3        27  0.0544  20.6644  0.7332  16.6296  1.2492  28.7328  0.1628   \n",
      "4   36.0932  2.5424  16.4332  4.0428  19.9932  3.2904  37.2168  2.1788   \n",
      "5   37.3064  2.1412  16.1996   2.732    18.93   2.532  37.3068  2.1996   \n",
      "6   38.0836  2.2836  16.2832  3.0152    20.07   2.392    37.03  2.1228   \n",
      "7     28.95  0.0716   7.7492  0.6544   8.7676  0.3268  29.0532   0.162   \n",
      "8   29.0764  0.1092   4.7564  0.3556  10.8852  0.4852  28.8096  0.2224   \n",
      "9        48       0     5.76  0.1608   1.7836  0.6988       48       0   \n",
      "10    41.98  0.0344  17.3244  0.7272   9.0096   1.492    42.01  0.0172   \n",
      "11    47.51  0.6532   22.672  0.4244    13.63  1.0572       48       0   \n",
      "12    45.47   0.278  13.4164  0.9968    14.67   1.008  45.5132  0.4704   \n",
      "13  46.4636  0.4776    12.06  0.2296  18.7744  0.6916  46.6728   0.444   \n",
      "14   42.536  0.5644  13.6932    0.48  15.5036  0.7884  43.0632  0.4956   \n",
      "15  44.6936  0.3456   6.9064  1.2156  14.5968  0.8424  42.4668  0.6228   \n",
      "16  34.8112  4.0764  16.6168  2.8912    16.04   3.856  34.7768  3.2976   \n",
      "17   34.384  4.5216  15.5364  3.1164  16.1708  3.8424  33.6532  4.3672   \n",
      "18  32.9328  5.0708  16.7768  2.9352  16.7736  3.4108    32.94  4.7668   \n",
      "\n",
      "        8       9    ...    332   333    334   335    336    337    338   339  \\\n",
      "0     18.05  0.6504  ...     26  3.27  37.75   1.5     44      1     26  1.12   \n",
      "1   21.5332  0.7564  ...  26.75  5.72  33.75   1.3   45.5   1.12  25.75  4.55   \n",
      "2   20.6232    0.66  ...     24  0.71  23.75  1.25     27   0.49     24  0.87   \n",
      "3    16.984  1.4324  ...  32.75  4.64     33  6.68     24   1.22     21  2.06   \n",
      "4   17.8416  2.7312  ...     21  4.71     23  8.17  42.67   7.25  22.33  7.26   \n",
      "5   15.3764  3.7296  ...  23.75   5.8  23.25   5.2  42.33   3.57   23.5  7.69   \n",
      "6   17.2668  2.7236  ...     23  4.97  20.33   8.5  42.75   7.04     24  7.12   \n",
      "7    5.9856  1.1516  ...     13  5.02   16.5   3.3     30   0.94  10.75  2.35   \n",
      "8    6.2136  0.5804  ...   7.33  0.71      4  1.25     48      0   6.75  0.83   \n",
      "9    6.3436  0.3844  ...   6.25  0.94   11.5  1.12     48      0   9.75  1.58   \n",
      "10  15.7136  0.4852  ...  18.25  3.56     20  4.78  44.75   0.94   22.5   3.3   \n",
      "11  15.1164   0.738  ...     22  2.55  20.25  5.89     46    0.5     21  4.58   \n",
      "12  13.6992   0.662  ...     19   3.9   21.5   1.3     45      0   11.5  3.19   \n",
      "13  14.1008   0.822  ...  18.25  4.76  18.25  3.27  43.67      1  16.75  0.87   \n",
      "14    16.42  0.4996  ...     15   1.3  18.33   1.3     45   0.83     12     3   \n",
      "15  14.4932  0.7052  ...  20.25  3.49  18.67  3.74     42   1.25     21  1.22   \n",
      "16    14.33  3.5828  ...  20.67  8.26  23.25  8.86  42.33   9.68  21.75   6.5   \n",
      "17  16.9636  3.0916  ...   19.5   7.4     20  6.48  43.25   7.26  18.75  6.53   \n",
      "18  14.3132  3.1272  ...   19.5  6.48  22.33  7.41   41.5  11.03     20  7.22   \n",
      "\n",
      "      340   341  \n",
      "0      32  0.94  \n",
      "1    34.5   1.5  \n",
      "2    22.5  1.12  \n",
      "3      29  4.74  \n",
      "4    23.5  7.79  \n",
      "5    23.5  8.83  \n",
      "6   20.67  8.26  \n",
      "7   11.75  1.92  \n",
      "8    2.33     1  \n",
      "9    11.5  1.22  \n",
      "10     28  5.79  \n",
      "11  20.75  7.22  \n",
      "12     22  1.09  \n",
      "13   16.5  1.12  \n",
      "14     18  1.64  \n",
      "15   19.5  2.06  \n",
      "16     24  6.69  \n",
      "17  22.25  7.95  \n",
      "18  21.33  8.04  \n",
      "\n",
      "[19 rows x 342 columns]\n",
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     1\n",
      "5     1\n",
      "6     1\n",
      "7     2\n",
      "8     2\n",
      "9     2\n",
      "10    3\n",
      "11    3\n",
      "12    3\n",
      "13    4\n",
      "14    4\n",
      "15    4\n",
      "16    5\n",
      "17    5\n",
      "18    5\n",
      "Name: Output, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_test = total_test.drop(columns=['Output'])\n",
    "y_test = total_test['Output']\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     1\n",
      "5     1\n",
      "6     1\n",
      "7     2\n",
      "8     2\n",
      "9     2\n",
      "10    3\n",
      "11    3\n",
      "12    3\n",
      "13    4\n",
      "14    4\n",
      "15    4\n",
      "16    5\n",
      "17    5\n",
      "18    5\n",
      "Name: Output, dtype: int64\n",
      "[0 0 0 0 1 1 1 2 2 2 4 3 3 4 4 4 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "cls_new = MultinomialNB().fit(X_train, y_train)\n",
    "y_test_predicted = cls_new.predict(X_test)    \n",
    "print(y_test)\n",
    "print(y_test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual test error is : \n",
      " 5.263157894736842 %\n"
     ]
    }
   ],
   "source": [
    "wrng = 0\n",
    "for q in np.arange(0,y_test.shape[0]):\n",
    "    if y_test_predicted[q] != y_test.iloc[q]:\n",
    "        wrng = wrng + 1\n",
    "\n",
    "wrngper = (wrng/(y_test.shape[0]))*100\n",
    "print('The actual test error is : \\n',wrngper,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test error is obtained the lowest for Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
